\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\BKM@entry[2]{}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\BKM@entry{id=1,dest={636861707465722E31},srcline={8}}{496E74726F64756374696F6E}
\citation{small2013lexical}
\citation{koehn2017six}
\citation{rios-etal-2018-word}
\citation{miller-1985-dictionaries}
\citation{45801,edunov-etal-2018-understanding}
\citation{NIPS2017_7278}
\citation{sennrich-haddow-birch:2016:P16-11}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:introduction}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{brf}{\backcite{small2013lexical}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{koehn2017six}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{rios-etal-2018-word}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{miller-1985-dictionaries}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{45801}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{edunov-etal-2018-understanding}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{NIPS2017_7278}{{1}{1}{chapter.1}}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{1}{1}{chapter.1}}}
\citation{DBLP:journals/corr/abs-2004-02181}
\BKM@entry{id=2,dest={73656374696F6E2E312E31},srcline={3}}{5265736561726368206F75746C696E6520616E64207175657374696F6E73}
\@writefile{brf}{\backcite{DBLP:journals/corr/abs-2004-02181}{{2}{1}{chapter.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Research outline and questions}{2}{section.1.1}\protected@file@percent }
\newlabel{section:introduction:rqs}{{1.1}{2}{Research outline and questions}{section.1.1}{}}
\citation{boyd-graber-etal-2007-topic,li-etal-2010-topic,ChaplotS18}
\citation{mikolov2013efficient}
\newacro{rq:topic}[\ref  {rq:topic}]{\textit  {Can document-level topic distribution help infer the meaning of a word?}}
\newacro{rq:topic1}[\ref  {rq:topic1}]{\textit  { To what extent can distributions over word senses be approximated by distributions over topics of documents without assuming these concepts to be identical?}}
\newacro{rq:topic2}[\ref  {rq:topic2}]{\textit  { How can we exploit document-level topics to distinguish between different meanings of a word and learn the corresponding representations?}}
\newacro{rq:topic3}[\ref  {rq:topic3}]{\textit  { What are the advantages of using document-level topics in learning multiple representations per word? }}
\newacro{rq:tdabt}[\ref  {rq:tdabt}]{\textit  {How is the translation quality of a word influenced by the availability of diverse contexts?}}
\newacro{rq:tda1}[\ref  {rq:tda1}]{\textit  {How can we successfully augment the training data with diverse contexts for rare words?}}
\newacro{rq:tda2}[\ref  {rq:tda2}]{\textit  {Do rare words benefit from augmentation via paraphrasing during test time?}}
\newacro{rq:bt1}[\ref  {rq:bt1}]{\textit  { Do signals from the NMT model help identify low-confidence words that could benefit from additional context? }}
\newacro{rq:bt2}[\ref  {rq:bt2}]{\textit  { How can we successfully apply data selection of monolingual data to diversify the contexts of low-confidence words? }}
\newacro{rq:vol}[\ref  {rq:vol}]{\textit  {To what extent are neural translation models vulnerable as a result of relying on the observed context in the training data to infer meaning? }}
\newacro{rq:id1}[\ref  {rq:id1}]{\textit  {What are the challenges of idiom translation with neural models? }}
\newacro{rq:id2}[\ref  {rq:id2}]{\textit  {How is the translation quality of NMT influenced by idiomatic expressions? }}
\newacro{rq:vol1}[\ref  {rq:vol1}]{\textit  { How can contextual modifications during testing reveal a lack of robustness of translation models and affect the translation quality?}}
\newacro{rq:vol2}[\ref  {rq:vol2}]{\textit  { To what extent is a lack of robustness an indicator of a generalization problem in neural machine translation models? }}
\acronymused{rq:topic}
\newlabel{rq:topic}{{{RQ1}}{3}{Research outline and questions}{Item.1}{}}
\acronymused{rq:topic1}
\newlabel{rq:topic1}{{{{\textbf  {RQ1.1}}}}{3}{Research outline and questions}{Item.2}{}}
\@writefile{brf}{\backcite{boyd-graber-etal-2007-topic}{{3}{{{\textbf  {RQ1.1}}}}{Item.2}}}
\@writefile{brf}{\backcite{li-etal-2010-topic}{{3}{{{\textbf  {RQ1.1}}}}{Item.2}}}
\@writefile{brf}{\backcite{ChaplotS18}{{3}{{{\textbf  {RQ1.1}}}}{Item.2}}}
\acronymused{rq:topic2}
\newlabel{rq:topic2}{{{{\textbf  {RQ1.2}}}}{3}{Research outline and questions}{Item.3}{}}
\@writefile{brf}{\backcite{mikolov2013efficient}{{3}{{{\textbf  {RQ1.2}}}}{Item.3}}}
\citation{ngo-etal-2019-overcoming}
\citation{koehn2017six}
\citation{luong2014addressing}
\acronymused{rq:topic3}
\newlabel{rq:topic3}{{{{\textbf  {RQ1.3}}}}{4}{Research outline and questions}{Item.4}{}}
\acronymused{rq:tdabt}
\newlabel{rq:tdabt}{{{RQ2}}{4}{Research outline and questions}{Item.5}{}}
\acronymused{rq:tda1}
\newlabel{rq:tda1}{{{{\textbf  {RQ2.1}}}}{4}{Research outline and questions}{Item.6}{}}
\@writefile{brf}{\backcite{ngo-etal-2019-overcoming}{{4}{{{\textbf  {RQ2.1}}}}{Item.6}}}
\@writefile{brf}{\backcite{koehn2017six}{{4}{{{\textbf  {RQ2.1}}}}{Item.6}}}
\@writefile{brf}{\backcite{luong2014addressing}{{4}{{{\textbf  {RQ2.1}}}}{Item.6}}}
\citation{sennrich-haddow-birch:2016:P16-11}
\acronymused{rq:tda2}
\newlabel{rq:tda2}{{{{\textbf  {RQ2.2}}}}{5}{Research outline and questions}{Item.7}{}}
\acronymused{rq:bt1}
\newlabel{rq:bt1}{{{{\textbf  {RQ2.3}}}}{5}{Research outline and questions}{Item.8}{}}
\acronymused{rq:bt2}
\newlabel{rq:bt2}{{{{\textbf  {RQ2.4}}}}{5}{Research outline and questions}{Item.9}{}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{5}{{{\textbf  {RQ2.4}}}}{Item.9}}}
\citation{sennrich-etal-2016-controlling}
\citation{goodfellow6572explaining,D18-1050,DBLP:journals/corr/abs-1711-02173}
\acronymused{rq:vol}
\newlabel{rq:vol}{{{RQ3}}{6}{Research outline and questions}{Item.10}{}}
\acronymused{rq:id1}
\newlabel{rq:id1}{{{{\textbf  {RQ3.1}}}}{6}{Research outline and questions}{Item.11}{}}
\acronymused{rq:id2}
\newlabel{rq:id2}{{{{\textbf  {RQ3.2}}}}{6}{Research outline and questions}{Item.12}{}}
\@writefile{brf}{\backcite{sennrich-etal-2016-controlling}{{6}{{{\textbf  {RQ3.2}}}}{Item.12}}}
\acronymused{rq:vol1}
\newlabel{rq:vol1}{{{{\textbf  {RQ3.3}}}}{6}{Research outline and questions}{Item.13}{}}
\@writefile{brf}{\backcite{goodfellow6572explaining}{{6}{{{\textbf  {RQ3.3}}}}{Item.13}}}
\BKM@entry{id=3,dest={73656374696F6E2E312E32},srcline={3}}{4D61696E20636F6E747269627574696F6E73}
\BKM@entry{id=4,dest={73756273656374696F6E2E312E322E31},srcline={8}}{416C676F726974686D696320636F6E747269627574696F6E73}
\@writefile{brf}{\backcite{D18-1050}{{7}{{{\textbf  {RQ3.3}}}}{Item.13}}}
\@writefile{brf}{\backcite{DBLP:journals/corr/abs-1711-02173}{{7}{{{\textbf  {RQ3.3}}}}{Item.13}}}
\acronymused{rq:vol2}
\newlabel{rq:vol2}{{{{\textbf  {RQ3.4}}}}{7}{Research outline and questions}{Item.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Main contributions}{7}{section.1.2}\protected@file@percent }
\newlabel{section:introduction:contributions}{{1.2}{7}{Main contributions}{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Algorithmic contributions}{7}{subsection.1.2.1}\protected@file@percent }
\BKM@entry{id=5,dest={73756273656374696F6E2E312E322E32},srcline={25}}{456D7069726963616C20636F6E747269627574696F6E73}
\BKM@entry{id=6,dest={73756273656374696F6E2E312E322E33},srcline={49}}{5265736F7572636520636F6E747269627574696F6E73}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Empirical contributions}{8}{subsection.1.2.2}\protected@file@percent }
\BKM@entry{id=7,dest={73656374696F6E2E312E33},srcline={3}}{546865736973206F76657276696577}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Resource contributions}{9}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Thesis overview}{9}{section.1.3}\protected@file@percent }
\newlabel{section:introduction:overview}{{1.3}{9}{Thesis overview}{section.1.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Chapter\relax \nobreakspace  {}\ref  {chapter:background}: Background}{9}{paragraph*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Chapter\relax \nobreakspace  {}\ref  {chapter:research-01}: Representation learning using documental context }{9}{paragraph*.3}\protected@file@percent }
\BKM@entry{id=8,dest={73656374696F6E2E312E34},srcline={3}}{4F726967696E73}
\citation{fadaee-etal-2017-learning}
\@writefile{toc}{\contentsline {paragraph}{Chapter\relax \nobreakspace  {}\ref  {chapter:research-02}: Data augmentation for rare words}{10}{paragraph*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Chapter\relax \nobreakspace  {}\ref  {chapter:research-03}: Data augmentation based on model failure}{10}{paragraph*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Chapter\relax \nobreakspace  {}\ref  {chapter:research-04}: Analyzing idiomatic expressions}{10}{paragraph*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Chapter\relax \nobreakspace  {}\ref  {chapter:research-05}: Analyzing volatility}{10}{paragraph*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Chapter\relax \nobreakspace  {}\ref  {chapter:conclusions}: Conclusion}{10}{paragraph*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Origins}{10}{section.1.4}\protected@file@percent }
\newlabel{section:introduction:origins}{{1.4}{10}{Origins}{section.1.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Chapter\relax \nobreakspace  {}\ref  {chapter:research-01}}{10}{paragraph*.9}\protected@file@percent }
\citation{fadaee-bisazza-monz:2017:Short2}
\citation{fadaee-monz-2018-back}
\citation{L18-1148}
\citation{fadaee_new}
\@writefile{brf}{\backcite{fadaee-etal-2017-learning}{{11}{1.4}{paragraph*.9}}}
\@writefile{toc}{\contentsline {paragraph}{Chapter\relax \nobreakspace  {}\ref  {chapter:research-02}}{11}{paragraph*.10}\protected@file@percent }
\@writefile{brf}{\backcite{fadaee-bisazza-monz:2017:Short2}{{11}{1.4}{paragraph*.10}}}
\@writefile{toc}{\contentsline {paragraph}{Chapter\relax \nobreakspace  {}\ref  {chapter:research-03}}{11}{paragraph*.11}\protected@file@percent }
\@writefile{brf}{\backcite{fadaee-monz-2018-back}{{11}{1.4}{paragraph*.11}}}
\@writefile{toc}{\contentsline {paragraph}{Chapter\relax \nobreakspace  {}\ref  {chapter:research-04}}{11}{paragraph*.12}\protected@file@percent }
\@writefile{brf}{\backcite{L18-1148}{{11}{1.4}{paragraph*.12}}}
\@writefile{toc}{\contentsline {paragraph}{Chapter\relax \nobreakspace  {}\ref  {chapter:research-05}}{11}{paragraph*.13}\protected@file@percent }
\@writefile{brf}{\backcite{fadaee_new}{{11}{1.4}{paragraph*.13}}}
\BKM@entry{id=9,dest={636861707465722E32},srcline={4}}{4261636B67726F756E64}
\citation{koehn-etal-2003-statistical}
\BKM@entry{id=10,dest={73656374696F6E2E322E31},srcline={1}}{506172616C6C656C20616E64206D6F6E6F6C696E6775616C20636F72706F7261}
\citation{koehn2005europarl}
\citation{steinberger2006jrc}
\citation{callisonburch-EtAl:2007:WMT}
\citation{barrault-EtAl:2019:WMT}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{13}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:background}{{2}{13}{Background}{chapter.2}{}}
\newlabel{nmtsec}{{2}{13}{Background}{chapter.2}{}}
\@writefile{brf}{\backcite{koehn-etal-2003-statistical}{{13}{2}{chapter.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Parallel and monolingual corpora}{13}{section.2.1}\protected@file@percent }
\newlabel{databg}{{2.1}{13}{Parallel and monolingual corpora}{section.2.1}{}}
\@writefile{brf}{\backcite{koehn2005europarl}{{13}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{steinberger2006jrc}{{13}{2.1}{section.2.1}}}
\citation{koehn-etal-2003-statistical,brants-etal-2007-large,koehn-etal-2007-moses}
\citation{lembersky-etal-2011-language}
\citation{wubben-etal-2012-sentence,kajiwara-komachi-2016-building}
\citation{zhang-lapata-2017-sentence}
\citation{domhan-hieber-2017-using,burlot-yvon-2018-using,currey-heafield-2019-zero}
\citation{currey2017copied}
\BKM@entry{id=11,dest={73756273656374696F6E2E322E312E31},srcline={27}}{4261636B2D5472616E736C6174696F6E20696E206D616368696E65207472616E736C6174696F6E}
\citation{Schwenk2008InvestigationsOL}
\citation{Rapp:2009:BSA:1667583.1667625}
\citation{tiedemann-etal-2016-phrase}
\@writefile{brf}{\backcite{callisonburch-EtAl:2007:WMT}{{14}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{barrault-EtAl:2019:WMT}{{14}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{koehn-etal-2003-statistical}{{14}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{brants-etal-2007-large}{{14}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{koehn-etal-2007-moses}{{14}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{lembersky-etal-2011-language}{{14}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{wubben-etal-2012-sentence}{{14}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{kajiwara-komachi-2016-building}{{14}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{zhang-lapata-2017-sentence}{{14}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{domhan-hieber-2017-using}{{14}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{burlot-yvon-2018-using}{{14}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{currey-heafield-2019-zero}{{14}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{currey2017copied}{{14}{2.1}{section.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Back-Translation in machine translation}{14}{subsection.2.1.1}\protected@file@percent }
\newlabel{bgbtref}{{2.1.1}{14}{Back-Translation in machine translation}{subsection.2.1.1}{}}
\@writefile{brf}{\backcite{Schwenk2008InvestigationsOL}{{14}{2.1.1}{subsection.2.1.1}}}
\@writefile{brf}{\backcite{Rapp:2009:BSA:1667583.1667625}{{14}{2.1.1}{subsection.2.1.1}}}
\citation{sennrich-haddow-birch:2016:P16-11}
\citation{pham2017karlsruhe}
\citation{axelrod2015class}
\citation{sennrich-haddow-birch:2016:P16-11}
\citation{edunov-etal-2018-understanding}
\citation{brants-etal-2007-large}
\citation{Ueffing2007}
\citation{NIPS2016_6469}
\citation{gulcehre2017integrating}
\BKM@entry{id=12,dest={73656374696F6E2E322E32},srcline={61}}{5472616E736C6174696F6E20766F636162756C617279}
\citation{DBLP:journals/corr/BahdanauCB14}
\citation{sutskever2014sequence}
\citation{cho2014properties}
\@writefile{brf}{\backcite{tiedemann-etal-2016-phrase}{{15}{2.1.1}{subsection.2.1.1}}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{15}{2.1.1}{subsection.2.1.1}}}
\@writefile{brf}{\backcite{pham2017karlsruhe}{{15}{2.1.1}{subsection.2.1.1}}}
\@writefile{brf}{\backcite{axelrod2015class}{{15}{2.1.1}{subsection.2.1.1}}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{15}{2.1.1}{subsection.2.1.1}}}
\@writefile{brf}{\backcite{edunov-etal-2018-understanding}{{15}{2.1.1}{subsection.2.1.1}}}
\@writefile{brf}{\backcite{brants-etal-2007-large}{{15}{2.1.1}{subsection.2.1.1}}}
\@writefile{brf}{\backcite{Ueffing2007}{{15}{2.1.1}{subsection.2.1.1}}}
\@writefile{brf}{\backcite{NIPS2016_6469}{{15}{2.1.1}{subsection.2.1.1}}}
\@writefile{brf}{\backcite{gulcehre2017integrating}{{15}{2.1.1}{subsection.2.1.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Translation vocabulary}{15}{section.2.2}\protected@file@percent }
\newlabel{bgvocab}{{2.2}{15}{Translation vocabulary}{section.2.2}{}}
\citation{jean-etal-2015-using}
\citation{luong2014addressing}
\citation{sennrich-haddow-birch:2016:P16-12}
\citation{10.5555/177910.177914}
\BKM@entry{id=13,dest={73656374696F6E2E322E33},srcline={1}}{576F726420726570726573656E746174696F6E73}
\citation{baroni-etal-2014-dont,baroni-lenci-2010-distributional}
\citation{firth1957synopsis}
\citation{Wang2019UsingDE}
\BKM@entry{id=14,dest={73756273656374696F6E2E322E332E31},srcline={12}}{53746174696320656D62656464696E6773}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\@writefile{brf}{\backcite{DBLP:journals/corr/BahdanauCB14}{{16}{2.2}{section.2.2}}}
\@writefile{brf}{\backcite{sutskever2014sequence}{{16}{2.2}{section.2.2}}}
\@writefile{brf}{\backcite{cho2014properties}{{16}{2.2}{section.2.2}}}
\@writefile{brf}{\backcite{jean-etal-2015-using}{{16}{2.2}{section.2.2}}}
\@writefile{brf}{\backcite{luong2014addressing}{{16}{2.2}{section.2.2}}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-12}{{16}{2.2}{section.2.2}}}
\@writefile{brf}{\backcite{10.5555/177910.177914}{{16}{2.2}{section.2.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Word representations}{16}{section.2.3}\protected@file@percent }
\newlabel{bgemb}{{2.3}{16}{Word representations}{section.2.3}{}}
\@writefile{brf}{\backcite{baroni-etal-2014-dont}{{16}{2.3}{section.2.3}}}
\@writefile{brf}{\backcite{baroni-lenci-2010-distributional}{{16}{2.3}{section.2.3}}}
\@writefile{brf}{\backcite{firth1957synopsis}{{16}{2.3}{section.2.3}}}
\@writefile{brf}{\backcite{Wang2019UsingDE}{{16}{2.3}{section.2.3}}}
\citation{pennington2014glove}
\citation{mikolov-etal-2013-linguistic}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{bgsgfig}{{2.1a}{17}{Skipgram model\relax }{figure.caption.14}{}}
\newlabel{sub@bgsgfig}{{a}{17}{Skipgram model\relax }{figure.caption.14}{}}
\newlabel{bgcbowfig}{{2.1b}{17}{CBOW model\relax }{figure.caption.14}{}}
\newlabel{sub@bgcbowfig}{{b}{17}{CBOW model\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Representation learning architectures proposed by \citet  {mikolov2013efficient}. The CBOW model predicts the current word given the context, and the Skipgram model predicts the surrounding words given the current word\relax }}{17}{figure.caption.14}\protected@file@percent }
\@writefile{brf}{\backcite{mikolov2013efficient}{{17}{2.1}{figure.caption.14}}}
\newlabel{bgsgcbowfig}{{2.1}{17}{Representation learning architectures proposed by \citet {mikolov2013efficient}. The CBOW model predicts the current word given the context, and the Skipgram model predicts the surrounding words given the current word\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Static embeddings}{17}{subsection.2.3.1}\protected@file@percent }
\newlabel{bgembstatic}{{2.3.1}{17}{Static embeddings}{subsection.2.3.1}{}}
\@writefile{brf}{\backcite{mikolov2013efficient}{{17}{2.3.1}{subsection.2.3.1}}}
\@writefile{brf}{\backcite{pennington2014glove}{{17}{2.3.1}{equation.2.3.2}}}
\citation{levy2014dependency}
\BKM@entry{id=15,dest={73756273656374696F6E2E322E332E32},srcline={65}}{44796E616D696320656D62656464696E6773}
\citation{peters-etal-2018-deep}
\citation{devlin-etal-2019-bert}
\citation{zhang2019semanticsaware,garg2019tanda,Lan2020ALBERT:,mandar2020}
\citation{wu2016google}
\citation{qi-etal-2018-pre}
\citation{lewis2019bart}
\@writefile{brf}{\backcite{mikolov-etal-2013-linguistic}{{18}{2.3.1}{equation.2.3.2}}}
\@writefile{brf}{\backcite{levy2014dependency}{{18}{2.3.1}{equation.2.3.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Dynamic embeddings}{18}{subsection.2.3.2}\protected@file@percent }
\@writefile{brf}{\backcite{peters-etal-2018-deep}{{18}{2.3.2}{subsection.2.3.2}}}
\@writefile{brf}{\backcite{devlin-etal-2019-bert}{{18}{2.3.2}{subsection.2.3.2}}}
\@writefile{brf}{\backcite{zhang2019semanticsaware}{{18}{2.3.2}{subsection.2.3.2}}}
\@writefile{brf}{\backcite{garg2019tanda}{{18}{2.3.2}{subsection.2.3.2}}}
\@writefile{brf}{\backcite{Lan2020ALBERT:}{{18}{2.3.2}{subsection.2.3.2}}}
\@writefile{brf}{\backcite{mandar2020}{{18}{2.3.2}{subsection.2.3.2}}}
\@writefile{brf}{\backcite{wu2016google}{{18}{2.3.2}{subsection.2.3.2}}}
\@writefile{brf}{\backcite{qi-etal-2018-pre}{{18}{2.3.2}{subsection.2.3.2}}}
\@writefile{brf}{\backcite{lewis2019bart}{{18}{2.3.2}{subsection.2.3.2}}}
\BKM@entry{id=16,dest={73656374696F6E2E322E34},srcline={1}}{526563757272656E74207472616E736C6174696F6E206D6F64656C73}
\citation{Jacquemin1994ATC,Schmidhuber:HabilitationThesis}
\citation{10.5555/65669.104451}
\citation{garg2019tanda}
\citation{Zhang2020RetrospectiveRF}
\citation{yuan2019objectcontextual}
\citation{8049322}
\citation{hochreiter1997long}
\citation{10.1142/S0218488598000094}
\citation{sutskever2014sequence}
\citation{cho2014properties}
\citation{DBLP:journals/corr/BahdanauCB14}
\citation{luong:2015:EMNLP}
\BKM@entry{id=17,dest={73756273656374696F6E2E322E342E31},srcline={25}}{4D6F64656C20617263686974656374757265}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Recurrent translation models}{19}{section.2.4}\protected@file@percent }
\newlabel{RNN}{{2.4}{19}{Recurrent translation models}{section.2.4}{}}
\@writefile{brf}{\backcite{Jacquemin1994ATC}{{19}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{Schmidhuber:HabilitationThesis}{{19}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{10.5555/65669.104451}{{19}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{garg2019tanda}{{19}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{Zhang2020RetrospectiveRF}{{19}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{yuan2019objectcontextual}{{19}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{8049322}{{19}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{hochreiter1997long}{{19}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{10.1142/S0218488598000094}{{19}{2.4}{section.2.4}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces An illustration of an RNN encoder-decoder with attention.\relax }}{19}{figure.caption.15}\protected@file@percent }
\newlabel{bgRNNfig}{{2.2}{19}{An illustration of an RNN encoder-decoder with attention.\relax }{figure.caption.15}{}}
\@writefile{brf}{\backcite{sutskever2014sequence}{{19}{2.4}{figure.caption.15}}}
\@writefile{brf}{\backcite{cho2014properties}{{19}{2.4}{figure.caption.15}}}
\@writefile{brf}{\backcite{DBLP:journals/corr/BahdanauCB14}{{19}{2.4}{figure.caption.15}}}
\@writefile{brf}{\backcite{luong:2015:EMNLP}{{19}{2.4}{figure.caption.15}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Model architecture}{20}{subsection.2.4.1}\protected@file@percent }
\citation{wu2016google}
\citation{britz-etal-2017-massive}
\citation{belinkov-etal-2017-neural}
\citation{luong:2015:EMNLP,DBLP:journals/corr/BahdanauCB14}
\citation{ghader-monz-2019-intrinsic}
\BKM@entry{id=18,dest={73756273656374696F6E2E322E342E32},srcline={88}}{496E666572656E6365}
\citation{jelinek98}
\citation{koehn-etal-2003-statistical}
\citation{sutskever2014sequence}
\citation{freitag-al-onaizan-2017-beam}
\citation{wu2016google,edunov-etal-2018-understanding}
\citation{wiseman-rush-2016-sequence,DBLP:journals/corr/RanzatoCAZ15}
\citation{collobert2019a}
\@writefile{brf}{\backcite{wu2016google}{{21}{2.4.1}{equation.2.4.8}}}
\@writefile{brf}{\backcite{britz-etal-2017-massive}{{21}{2.4.1}{equation.2.4.8}}}
\@writefile{brf}{\backcite{belinkov-etal-2017-neural}{{21}{2.4.1}{equation.2.4.8}}}
\@writefile{brf}{\backcite{luong:2015:EMNLP}{{21}{2.4.1}{equation.2.4.8}}}
\@writefile{brf}{\backcite{DBLP:journals/corr/BahdanauCB14}{{21}{2.4.1}{equation.2.4.8}}}
\@writefile{brf}{\backcite{ghader-monz-2019-intrinsic}{{21}{2.4.1}{equation.2.4.8}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Inference}{21}{subsection.2.4.2}\protected@file@percent }
\newlabel{bgrnninference}{{2.4.2}{21}{Inference}{subsection.2.4.2}{}}
\@writefile{brf}{\backcite{jelinek98}{{21}{2.4.2}{subsection.2.4.2}}}
\@writefile{brf}{\backcite{koehn-etal-2003-statistical}{{21}{2.4.2}{subsection.2.4.2}}}
\@writefile{brf}{\backcite{sutskever2014sequence}{{21}{2.4.2}{equation.2.4.9}}}
\@writefile{brf}{\backcite{freitag-al-onaizan-2017-beam}{{21}{2.4.2}{equation.2.4.9}}}
\BKM@entry{id=19,dest={73756273656374696F6E2E322E342E33},srcline={108}}{417474656E74696F6E206D656368616E69736D}
\citation{koehn2017six}
\citation{Graves2014NeuralTM,DBLP:journals/corr/BahdanauCB14,luong:2015:EMNLP}
\citation{DBLP:journals/corr/BahdanauCB14}
\citation{luong:2015:EMNLP}
\citation{luong:2015:EMNLP}
\citation{luong:2015:EMNLP}
\citation{vaswani2017attention}
\citation{ghader-monz-2017-attention,koehn2017six}
\BKM@entry{id=20,dest={73656374696F6E2E322E35},srcline={1}}{46756C6C7920617474656E74696F6E2D6261736564207472616E736C6174696F6E206D6F64656C73}
\citation{vaswani2017attention}
\citation{lakew-etal-2018-comparison}
\citation{edunov-etal-2018-understanding,aharoni-etal-2019-massively}
\@writefile{brf}{\backcite{DBLP:journals/corr/BahdanauCB14}{{22}{\caption@xref {??}{ on input line 129}}{table.caption.16}}}
\@writefile{brf}{\backcite{luong:2015:EMNLP}{{22}{\caption@xref {??}{ on input line 129}}{table.caption.16}}}
\@writefile{brf}{\backcite{luong:2015:EMNLP}{{22}{\caption@xref {??}{ on input line 129}}{table.caption.16}}}
\@writefile{brf}{\backcite{luong:2015:EMNLP}{{22}{\caption@xref {??}{ on input line 129}}{table.caption.16}}}
\@writefile{brf}{\backcite{vaswani2017attention}{{22}{\caption@xref {??}{ on input line 129}}{table.caption.16}}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Different alignment scores in the literature used for creating the context vector. \relax }}{22}{table.caption.16}\protected@file@percent }
\newlabel{backgroundattentions}{{2.1}{22}{Different alignment scores in the literature used for creating the context vector. \relax }{table.caption.16}{}}
\@writefile{brf}{\backcite{wu2016google}{{22}{2.4.2}{equation.2.4.9}}}
\@writefile{brf}{\backcite{edunov-etal-2018-understanding}{{22}{2.4.2}{equation.2.4.9}}}
\@writefile{brf}{\backcite{wiseman-rush-2016-sequence}{{22}{2.4.2}{equation.2.4.9}}}
\@writefile{brf}{\backcite{DBLP:journals/corr/RanzatoCAZ15}{{22}{2.4.2}{equation.2.4.9}}}
\@writefile{brf}{\backcite{collobert2019a}{{22}{2.4.2}{equation.2.4.9}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Attention mechanism}{22}{subsection.2.4.3}\protected@file@percent }
\newlabel{BGlstmATT}{{2.4.3}{22}{Attention mechanism}{subsection.2.4.3}{}}
\@writefile{brf}{\backcite{koehn2017six}{{22}{2.4.3}{subsection.2.4.3}}}
\@writefile{brf}{\backcite{Graves2014NeuralTM}{{22}{2.4.3}{subsection.2.4.3}}}
\@writefile{brf}{\backcite{DBLP:journals/corr/BahdanauCB14}{{22}{2.4.3}{subsection.2.4.3}}}
\@writefile{brf}{\backcite{luong:2015:EMNLP}{{22}{2.4.3}{subsection.2.4.3}}}
\@writefile{brf}{\backcite{ghader-monz-2017-attention}{{22}{2.4.3}{table.caption.16}}}
\@writefile{brf}{\backcite{koehn2017six}{{22}{2.4.3}{table.caption.16}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Fully attention-based translation models}{22}{section.2.5}\protected@file@percent }
\newlabel{TRNN}{{2.5}{22}{Fully attention-based translation models}{section.2.5}{}}
\BKM@entry{id=21,dest={73756273656374696F6E2E322E352E31},srcline={10}}{4D6F64656C20617263686974656374757265}
\citation{vaswani2017attention}
\citation{vaswani2017attention}
\citation{vaswani2017attention}
\citation{vaswani2017attention}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces An illustration of a Transformer model introduced in \citet  {vaswani2017attention}.\relax }}{23}{figure.caption.17}\protected@file@percent }
\@writefile{brf}{\backcite{vaswani2017attention}{{23}{2.3}{figure.caption.17}}}
\newlabel{bgTRNfig}{{2.3}{23}{An illustration of a Transformer model introduced in \citet {vaswani2017attention}.\relax }{figure.caption.17}{}}
\@writefile{brf}{\backcite{vaswani2017attention}{{23}{2.5}{section.2.5}}}
\@writefile{brf}{\backcite{lakew-etal-2018-comparison}{{23}{2.5}{section.2.5}}}
\@writefile{brf}{\backcite{edunov-etal-2018-understanding}{{23}{2.5}{section.2.5}}}
\@writefile{brf}{\backcite{aharoni-etal-2019-massively}{{23}{2.5}{section.2.5}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Model architecture}{23}{subsection.2.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces An illustration of self-attention in the Transformer model based on \citet  {vaswani2017attention}.\relax }}{24}{figure.caption.18}\protected@file@percent }
\@writefile{brf}{\backcite{vaswani2017attention}{{24}{2.4}{figure.caption.18}}}
\newlabel{bgTRNattfig}{{2.4}{24}{An illustration of self-attention in the Transformer model based on \citet {vaswani2017attention}.\relax }{figure.caption.18}{}}
\BKM@entry{id=22,dest={73756273656374696F6E2E322E352E32},srcline={68}}{526573696475616C20636F6E6E656374696F6E73}
\citation{He2016DeepRL}
\citation{Ba2016LayerN}
\BKM@entry{id=23,dest={73756273656374696F6E2E322E352E33},srcline={75}}{506F736974696F6E616C20456E636F64696E67}
\citation{tran-etal-2018-importance}
\citation{vaswani2017attention}
\citation{pmlr-v70-gehring17a}
\BKM@entry{id=24,dest={73656374696F6E2E322E36},srcline={1}}{5472616E736C6174696F6E206576616C756174696F6E}
\citation{Papineni2001}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Residual connections}{25}{subsection.2.5.2}\protected@file@percent }
\@writefile{brf}{\backcite{He2016DeepRL}{{25}{2.5.2}{subsection.2.5.2}}}
\@writefile{brf}{\backcite{Ba2016LayerN}{{25}{2.5.2}{subsection.2.5.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Positional Encoding}{25}{subsection.2.5.3}\protected@file@percent }
\@writefile{brf}{\backcite{tran-etal-2018-importance}{{25}{2.5.3}{subsection.2.5.3}}}
\@writefile{brf}{\backcite{vaswani2017attention}{{25}{2.5.3}{subsection.2.5.3}}}
\@writefile{brf}{\backcite{pmlr-v70-gehring17a}{{25}{2.5.3}{equation.2.5.13}}}
\citation{banerjee-lavie-2005-meteor,denkowski-lavie-2011-meteor,denkowski:lavie:meteor-wmt:2014}
\citation{agarwal-lavie-2008-meteor}
\citation{Snover06astudy}
\citation{callison-burch-etal-2006-evaluating}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Translation evaluation}{26}{section.2.6}\protected@file@percent }
\newlabel{bgexp}{{2.6}{26}{Translation evaluation}{section.2.6}{}}
\@writefile{brf}{\backcite{Papineni2001}{{26}{2.6}{section.2.6}}}
\@writefile{brf}{\backcite{banerjee-lavie-2005-meteor}{{26}{2.6}{equation.2.6.16}}}
\@writefile{brf}{\backcite{denkowski-lavie-2011-meteor}{{26}{2.6}{equation.2.6.16}}}
\@writefile{brf}{\backcite{denkowski:lavie:meteor-wmt:2014}{{26}{2.6}{equation.2.6.16}}}
\@writefile{brf}{\backcite{agarwal-lavie-2008-meteor}{{26}{2.6}{equation.2.6.16}}}
\@writefile{brf}{\backcite{Snover06astudy}{{26}{2.6}{equation.2.6.16}}}
\@writefile{brf}{\backcite{callison-burch-etal-2006-evaluating}{{26}{2.6}{equation.2.6.16}}}
\BKM@entry{id=25,dest={636861707465722E33},srcline={4}}{546F7069632D53656E73697469766520576F726420526570726573656E746174696F6E73}
\BKM@entry{id=26,dest={73656374696F6E2E332E31},srcline={7}}{496E74726F64756374696F6E20616E64207265736561726368207175657374696F6E73}
\citation{mikolov2013efficient,pennington2014glove}
\citation{tang2014learning,yu-etal-2017-refining}
\citation{salehi-etal-2015-word,gharbieh2016word}
\citation{zou2013bilingual,artetxe-etal-2018-unsupervised}
\citation{firth1957synopsis}
\citation{peters-etal-2018-deep}
\citation{devlin-etal-2019-bert}
\citation{Bengio:2003,reisinger-mooney:2010:NAACLHLT}
\citation{faruqui2016problems}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Topic-Sensitive Word Representations}{29}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:research-01}{{3}{29}{Topic-Sensitive Word Representations}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction and research questions}{29}{section.3.1}\protected@file@percent }
\@writefile{brf}{\backcite{mikolov2013efficient}{{29}{3.1}{section.3.1}}}
\@writefile{brf}{\backcite{pennington2014glove}{{29}{3.1}{section.3.1}}}
\@writefile{brf}{\backcite{tang2014learning}{{29}{3.1}{section.3.1}}}
\@writefile{brf}{\backcite{yu-etal-2017-refining}{{29}{3.1}{section.3.1}}}
\@writefile{brf}{\backcite{salehi-etal-2015-word}{{29}{3.1}{section.3.1}}}
\@writefile{brf}{\backcite{gharbieh2016word}{{29}{3.1}{section.3.1}}}
\@writefile{brf}{\backcite{zou2013bilingual}{{29}{3.1}{section.3.1}}}
\@writefile{brf}{\backcite{artetxe-etal-2018-unsupervised}{{29}{3.1}{section.3.1}}}
\@writefile{brf}{\backcite{firth1957synopsis}{{29}{3.1}{section.3.1}}}
\@writefile{brf}{\backcite{peters-etal-2018-deep}{{29}{3.1}{section.3.1}}}
\@writefile{brf}{\backcite{devlin-etal-2019-bert}{{29}{3.1}{section.3.1}}}
\@writefile{brf}{\backcite{Bengio:2003}{{29}{3.1}{section.3.1}}}
\@writefile{brf}{\backcite{reisinger-mooney:2010:NAACLHLT}{{29}{3.1}{section.3.1}}}
\citation{yao2011nonparametric}
\citation{mccarthy2007semeval}
\@writefile{brf}{\backcite{faruqui2016problems}{{30}{3.1}{section.3.1}}}
\@writefile{toc}{\contentsline {paragraph}{Research Question 1:}{30}{paragraph*.19}\protected@file@percent }
\acronymused{rq:topic}
\acronymused{rq:topic1}
\@writefile{brf}{\backcite{yao2011nonparametric}{{30}{{{\textbf  {RQ1.1 }}}}{Item.29}}}
\acronymused{rq:topic2}
\BKM@entry{id=27,dest={73656374696F6E2E332E32},srcline={88}}{52656C6174656420776F726B}
\BKM@entry{id=28,dest={73756273656374696F6E2E332E322E31},srcline={92}}{576F72642073656E736520646973616D626967756174696F6E}
\citation{ide-veronis-1998-introduction}
\citation{miller1995wordnet,navigli2010babelnet}
\citation{journals/lre/Kilgarriff97,Navigli2012}
\citation{brody-lapata:2009:EACL,lau2014learning}
\@writefile{brf}{\backcite{mccarthy2007semeval}{{31}{{{\textbf  {RQ1.2 }}}}{Item.30}}}
\acronymused{rq:topic3}
\@writefile{toc}{\contentsline {paragraph}{Organization.}{31}{paragraph*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Related work}{31}{section.3.2}\protected@file@percent }
\newlabel{toprel}{{3.2}{31}{Related work}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Word sense disambiguation}{31}{subsection.3.2.1}\protected@file@percent }
\@writefile{brf}{\backcite{ide-veronis-1998-introduction}{{31}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{miller1995wordnet}{{31}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{navigli2010babelnet}{{31}{3.2.1}{subsection.3.2.1}}}
\citation{yao2011nonparametric}
\citation{Jordan2011THEEO,6802355}
\BKM@entry{id=29,dest={73756273656374696F6E2E332E322E32},srcline={105}}{5374617469632073656E736520726570726573656E746174696F6E73}
\citation{mikolov2013efficient,pennington2014glove}
\citation{qiu-tu-yu:2016:EMNLP2016}
\citation{huang2012improving}
\citation{neelakantan2014efficient}
\citation{li-jurafsky:2015:EMNLP}
\citation{mikolov2013efficient}
\BKM@entry{id=30,dest={73656374696F6E2E332E33},srcline={118}}{546F7069632D53656E73697469766520726570726573656E746174696F6E73}
\citation{mikolov2013efficient}
\citation{Jordan2011THEEO,6802355}
\citation{teh2006hierarchical,lau2014learning}
\citation{citeulike:635668,brody-lapata:2009:EACL}
\@writefile{brf}{\backcite{journals/lre/Kilgarriff97}{{32}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{Navigli2012}{{32}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{brody-lapata:2009:EACL}{{32}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{lau2014learning}{{32}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{yao2011nonparametric}{{32}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{Jordan2011THEEO}{{32}{3.2.1}{subsection.3.2.1}}}
\@writefile{brf}{\backcite{6802355}{{32}{3.2.1}{subsection.3.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Static sense representations}{32}{subsection.3.2.2}\protected@file@percent }
\@writefile{brf}{\backcite{mikolov2013efficient}{{32}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{pennington2014glove}{{32}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{qiu-tu-yu:2016:EMNLP2016}{{32}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{huang2012improving}{{32}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{neelakantan2014efficient}{{32}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{li-jurafsky:2015:EMNLP}{{32}{3.2.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{mikolov2013efficient}{{32}{3.2.2}{subsection.3.2.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Topic-Sensitive representations}{32}{section.3.3}\protected@file@percent }
\newlabel{sect:model}{{3.3}{32}{Topic-Sensitive representations}{section.3.3}{}}
\@writefile{brf}{\backcite{mikolov2013efficient}{{32}{3.3}{section.3.3}}}
\@writefile{brf}{\backcite{Jordan2011THEEO}{{32}{3.3}{section.3.3}}}
\@writefile{brf}{\backcite{6802355}{{32}{3.3}{section.3.3}}}
\@writefile{brf}{\backcite{teh2006hierarchical}{{32}{3.3}{section.3.3}}}
\@writefile{brf}{\backcite{lau2014learning}{{32}{3.3}{section.3.3}}}
\citation{neelakantan2014efficient}
\citation{mikolov2013efficient}
\BKM@entry{id=31,dest={73756273656374696F6E2E332E332E31},srcline={155}}{4861726420746F7069632D6C6162656C656420726570726573656E746174696F6E73}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An example of sentences including the word `bat' from two documents with different topic distribution.\relax }}{33}{figure.caption.21}\protected@file@percent }
\newlabel{modelsfig0}{{3.1}{33}{An example of sentences including the word `bat' from two documents with different topic distribution.\relax }{figure.caption.21}{}}
\@writefile{brf}{\backcite{citeulike:635668}{{33}{3.3}{section.3.3}}}
\@writefile{brf}{\backcite{brody-lapata:2009:EACL}{{33}{3.3}{section.3.3}}}
\@writefile{brf}{\backcite{neelakantan2014efficient}{{33}{3.3}{figure.caption.21}}}
\@writefile{brf}{\backcite{mikolov2013efficient}{{33}{3.3}{figure.caption.21}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Hard topic-labeled representations}{33}{subsection.3.3.1}\protected@file@percent }
\newlabel{sect:hardReps}{{3.3.1}{33}{Hard topic-labeled representations}{subsection.3.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Illustrations of two proposed models with hard-labeling topics in this chapter. \relax }}{34}{figure.caption.22}\protected@file@percent }
\newlabel{modelsfigH}{{3.2}{34}{Illustrations of two proposed models with hard-labeling topics in this chapter. \relax }{figure.caption.22}{}}
\newlabel{eq:HTLE}{{3.2}{34}{Hard topic-labeled representations}{equation.3.3.2}{}}
\newlabel{eq:HTLEadd}{{3.3}{34}{Hard topic-labeled representations}{equation.3.3.3}{}}
\BKM@entry{id=32,dest={73756273656374696F6E2E332E332E32},srcline={210}}{536F667420746F7069632D6C6162656C656420726570726573656E746174696F6E73}
\citation{mikolov2013efficient}
\citation{pennington2014glove}
\citation{mikolov2013efficient}
\citation{pennington2014glove}
\BKM@entry{id=33,dest={73756273656374696F6E2E332E332E33},srcline={287}}{456D62656464696E677320666F7220706F6C7973656D6F757320776F726473}
\citation{mikolov2013distributed}
\citation{pennington2014glove}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Illustration of our soft topic-labeled representation model (STLE). \relax }}{35}{figure.caption.23}\protected@file@percent }
\newlabel{modelsfigS}{{3.3}{35}{Illustration of our soft topic-labeled representation model (STLE). \relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Soft topic-labeled representations}{35}{subsection.3.3.2}\protected@file@percent }
\newlabel{sect:softReps}{{3.3.2}{35}{Soft topic-labeled representations}{subsection.3.3.2}{}}
\newlabel{eq:STLE}{{3.4}{35}{Soft topic-labeled representations}{equation.3.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Embeddings for polysemous words}{35}{subsection.3.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Nearest neighbors of three examples in different representation spaces using cosine similarity. \textbf  {word2vec} and \textbf  {GloVe} are pre-trained embeddings from \citep {mikolov2013efficient} and \citep {pennington2014glove}, respectively. \textbf  {SGE} is the Skipgram baseline and \textbf  {HTLE} is our topic-sensitive Skipgram (cf. Equation\relax \nobreakspace  {}(\ref  {eq:HTLE})), both trained on Wikipedia. $\tau _k$ stands for HDP-inferred topic $k$.\relax }}{36}{table.caption.24}\protected@file@percent }
\@writefile{brf}{\backcite{mikolov2013efficient}{{36}{3.1}{table.caption.24}}}
\@writefile{brf}{\backcite{pennington2014glove}{{36}{3.1}{table.caption.24}}}
\newlabel{embs}{{3.1}{36}{Nearest neighbors of three examples in different representation spaces using cosine similarity. \textbf {word2vec} and \textbf {GloVe} are pre-trained embeddings from \protect \citep {mikolov2013efficient} and \protect \citep {pennington2014glove}, respectively. \textbf {SGE} is the Skipgram baseline and \textbf {HTLE} is our topic-sensitive Skipgram (cf. Equation~(\ref {eq:HTLE})), both trained on Wikipedia. $\tau _k$ stands for HDP-inferred topic $k$.\relax }{table.caption.24}{}}
\citation{miller1995wordnet}
\BKM@entry{id=34,dest={73656374696F6E2E332E34},srcline={331}}{4576616C756174696F6E}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Statistics of the degree of polysemy in Wordnet and HTLE. \relax }}{37}{table.caption.25}\protected@file@percent }
\newlabel{wordnetinfo}{{3.2}{37}{Statistics of the degree of polysemy in Wordnet and HTLE. \relax }{table.caption.25}{}}
\@writefile{brf}{\backcite{mikolov2013distributed}{{37}{3.3.3}{subsection.3.3.3}}}
\@writefile{brf}{\backcite{pennington2014glove}{{37}{3.3.3}{subsection.3.3.3}}}
\@writefile{brf}{\backcite{miller1995wordnet}{{37}{3.3.3}{table.caption.25}}}
\BKM@entry{id=35,dest={73756273656374696F6E2E332E342E31},srcline={335}}{4578706572696D656E74616C207365747570}
\citation{teh2006hierarchical}
\citation{Gale1992}
\citation{mikolov2013efficient}
\citation{levy2015improving}
\citation{Rubenstein:1965:CCS:365628.365657}
\citation{Finkelstein:2001:PSC:371920.372094}
\citation{Radinsky:2011:WTC:1963405.1963455}
\citation{bruni-etal-2012-distributional}
\citation{luong-etal-2013-better}
\citation{hill-etal-2015-simlex}
\BKM@entry{id=36,dest={73756273656374696F6E2E332E342E32},srcline={373}}{576F72642073696D696C6172697479207461736B}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Evaluation}{38}{section.3.4}\protected@file@percent }
\newlabel{embevaluationsec}{{3.4}{38}{Evaluation}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Experimental setup}{38}{subsection.3.4.1}\protected@file@percent }
\newlabel{embexpsetup}{{3.4.1}{38}{Experimental setup}{subsection.3.4.1}{}}
\@writefile{brf}{\backcite{teh2006hierarchical}{{38}{3.4.1}{subsection.3.4.1}}}
\@writefile{brf}{\backcite{Gale1992}{{38}{3.4.1}{subsection.3.4.1}}}
\@writefile{brf}{\backcite{mikolov2013efficient}{{38}{3.4.1}{subsection.3.4.1}}}
\@writefile{brf}{\backcite{levy2015improving}{{38}{3.4.1}{subsection.3.4.1}}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Word similarity benchmarks for intrinsic evaluation of word representations. \relax }}{38}{table.caption.26}\protected@file@percent }
\newlabel{wstab}{{3.3}{38}{Word similarity benchmarks for intrinsic evaluation of word representations. \relax }{table.caption.26}{}}
\@writefile{brf}{\backcite{Rubenstein:1965:CCS:365628.365657}{{38}{3.3}{table.caption.26}}}
\@writefile{brf}{\backcite{Finkelstein:2001:PSC:371920.372094}{{38}{3.3}{table.caption.26}}}
\@writefile{brf}{\backcite{Radinsky:2011:WTC:1963405.1963455}{{38}{3.3}{table.caption.26}}}
\@writefile{brf}{\backcite{bruni-etal-2012-distributional}{{38}{3.3}{table.caption.26}}}
\@writefile{brf}{\backcite{luong-etal-2013-better}{{38}{3.3}{table.caption.26}}}
\@writefile{brf}{\backcite{hill-etal-2015-simlex}{{38}{3.3}{table.caption.26}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Word similarity task}{38}{subsection.3.4.2}\protected@file@percent }
\citation{Radinsky:2011:WTC:1963405.1963455,hassan2011semantic,yih-qazvinian-2012-measuring}
\citation{faruqui2016problems,torabi-asr-etal-2018-querying}
\citation{bruni-etal-2012-distributional}
\citation{hill-etal-2015-simlex}
\@writefile{brf}{\backcite{Radinsky:2011:WTC:1963405.1963455}{{39}{3.4.2}{subsection.3.4.2}}}
\@writefile{brf}{\backcite{hassan2011semantic}{{39}{3.4.2}{subsection.3.4.2}}}
\@writefile{brf}{\backcite{yih-qazvinian-2012-measuring}{{39}{3.4.2}{subsection.3.4.2}}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Spearman's rank correlation performance on word similarity tasks. All vectors are 100-dimensional. \relax }}{39}{table.caption.27}\protected@file@percent }
\newlabel{wstabres}{{3.4}{39}{Spearman's rank correlation performance on word similarity tasks. All vectors are 100-dimensional. \relax }{table.caption.27}{}}
\citation{faruqui2016problems}
\citation{huang2012improving}
\BKM@entry{id=37,dest={73756273656374696F6E2E332E342E33},srcline={437}}{436F6E746578742D417761726520776F72642073696D696C6172697479207461736B}
\citation{huang2012improving}
\citation{peters-etal-2018-deep,devlin-etal-2019-bert}
\citation{neelakantan2014efficient}
\citation{neelakantan2014efficient}
\@writefile{brf}{\backcite{faruqui2016problems}{{40}{3.4.2}{table.caption.27}}}
\@writefile{brf}{\backcite{torabi-asr-etal-2018-querying}{{40}{3.4.2}{table.caption.27}}}
\@writefile{brf}{\backcite{bruni-etal-2012-distributional}{{40}{3.4.2}{table.caption.27}}}
\@writefile{brf}{\backcite{hill-etal-2015-simlex}{{40}{3.4.2}{table.caption.27}}}
\@writefile{brf}{\backcite{faruqui2016problems}{{40}{3.4.2}{table.caption.27}}}
\@writefile{brf}{\backcite{huang2012improving}{{40}{3.4.2}{table.caption.27}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Context-Aware word similarity task}{40}{subsection.3.4.3}\protected@file@percent }
\@writefile{brf}{\backcite{huang2012improving}{{40}{3.4.3}{subsection.3.4.3}}}
\@writefile{brf}{\backcite{peters-etal-2018-deep}{{40}{3.4.3}{subsection.3.4.3}}}
\@writefile{brf}{\backcite{devlin-etal-2019-bert}{{40}{3.4.3}{subsection.3.4.3}}}
\@writefile{brf}{\backcite{neelakantan2014efficient}{{40}{3.4.3}{equation.3.4.6}}}
\citation{huang2012improving}
\citation{huang2012improving}
\citation{mikolov2013efficient}
\citation{neelakantan2014efficient}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Examples from SCWS data set. Each example includes the word pair (identical or non-identical), their corresponding contexts, and the average human score between 0 and 10 to indicate the similarity. \relax }}{41}{table.caption.28}\protected@file@percent }
\newlabel{iwdtexample}{{3.5}{41}{Examples from SCWS data set. Each example includes the word pair (identical or non-identical), their corresponding contexts, and the average human score between 0 and 10 to indicate the similarity. \relax }{table.caption.28}{}}
\@writefile{brf}{\backcite{neelakantan2014efficient}{{41}{3.4.3}{equation.3.4.6}}}
\newlabel{mssgavgsim}{{3.7}{41}{Context-Aware word similarity task}{equation.3.4.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Spearman's rank correlation performance for the Word Similarity task on SCWS \citep  {huang2012improving}.\relax }}{42}{table.caption.29}\protected@file@percent }
\@writefile{brf}{\backcite{huang2012improving}{{42}{3.6}{table.caption.29}}}
\newlabel{scws}{{3.6}{42}{Spearman's rank correlation performance for the Word Similarity task on SCWS \citep {huang2012improving}.\relax }{table.caption.29}{}}
\@writefile{brf}{\backcite{mikolov2013efficient}{{42}{3.6}{table.caption.29}}}
\@writefile{brf}{\backcite{neelakantan2014efficient}{{42}{3.6}{table.caption.29}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Average absolute rank of the baseline embeddings and the HTLE embeddings in four categories, where being closer to the gold rankings is better. The categories are marked with a combination of these labels: \textbf  {I}: identical, \textbf  {D}: different. \textbf  {W}: word, \textbf  {T}: topic. For instance \texttt  {IWDT} is the category of word pairs where \textit  {\textbf  {i}dentical \textbf  {w}ords have \textbf  {d}ifferent \textbf  {t}opics}. Examples of the categories are presented in Table\relax \nobreakspace  {}\ref  {iwdtexample}.\relax }}{42}{figure.caption.30}\protected@file@percent }
\newlabel{scwsbar}{{3.4}{42}{Average absolute rank of the baseline embeddings and the HTLE embeddings in four categories, where being closer to the gold rankings is better. The categories are marked with a combination of these labels: \textbf {I}: identical, \textbf {D}: different. \textbf {W}: word, \textbf {T}: topic. For instance \texttt {IWDT} is the category of word pairs where \textit {\textbf {i}dentical \textbf {w}ords have \textbf {d}ifferent \textbf {t}opics}. Examples of the categories are presented in Table~\ref {iwdtexample}.\relax }{figure.caption.30}{}}
\BKM@entry{id=38,dest={73756273656374696F6E2E332E342E34},srcline={559}}{4C65786963616C20737562737469747574696F6E207461736B}
\citation{melamud2015simple}
\citation{mccarthy2007semeval}
\citation{kremer2014substitutes}
\citation{kremer2014substitutes}
\citation{DBLP:conf/emnlp/SzarvasBH13,kremer2014substitutes,melamud2015simple}
\citation{kishida2005property}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Lexical substitution task}{43}{subsection.3.4.4}\protected@file@percent }
\@writefile{brf}{\backcite{melamud2015simple}{{43}{3.4.4}{subsection.3.4.4}}}
\@writefile{brf}{\backcite{mccarthy2007semeval}{{43}{3.4.4}{subsection.3.4.4}}}
\@writefile{brf}{\backcite{kremer2014substitutes}{{43}{3.4.4}{subsection.3.4.4}}}
\@writefile{brf}{\backcite{kremer2014substitutes}{{43}{3.4.4}{subsection.3.4.4}}}
\@writefile{brf}{\backcite{DBLP:conf/emnlp/SzarvasBH13}{{43}{3.4.4}{subsection.3.4.4}}}
\@writefile{brf}{\backcite{kremer2014substitutes}{{43}{3.4.4}{subsection.3.4.4}}}
\@writefile{brf}{\backcite{melamud2015simple}{{43}{3.4.4}{subsection.3.4.4}}}
\@writefile{brf}{\backcite{kishida2005property}{{43}{3.4.4}{subsection.3.4.4}}}
\citation{melamud2015simple}
\@writefile{brf}{\backcite{melamud2015simple}{{44}{3.4.4}{equation.3.4.8}}}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces GAP scores on LS-SE07 and LS-CIC sets. For \textsc  {SGE + context} we use the \textit  {context} embeddings to disambiguate the substitutions. Improvements over the best baseline (MSSG) are marked $^\blacktriangle $ at $p<.01$ and $^\vartriangle $ at $p < .05$.\relax }}{44}{table.caption.31}\protected@file@percent }
\newlabel{embs_tables00}{{3.7}{44}{GAP scores on LS-SE07 and LS-CIC sets. For \textsc {SGE + context} we use the \textit {context} embeddings to disambiguate the substitutions. Improvements over the best baseline (MSSG) are marked $^\blacktriangle $ at $p<.01$ and $^\vartriangle $ at $p < .05$.\relax }{table.caption.31}{}}
\citation{neelakantan2014efficient}
\citation{sprent2016applied}
\citation{neelakantan2014efficient}
\citation{neelakantan2014efficient}
\citation{li-jurafsky:2015:EMNLP}
\@writefile{brf}{\backcite{neelakantan2014efficient}{{45}{3.4.4}{equation.3.4.11}}}
\@writefile{brf}{\backcite{sprent2016applied}{{45}{3.4.4}{equation.3.4.11}}}
\@writefile{brf}{\backcite{li-jurafsky:2015:EMNLP}{{45}{3.4.4}{table.caption.33}}}
\BKM@entry{id=39,dest={73656374696F6E2E332E35},srcline={764}}{5175616C6974617469766520616E616C79736973}
\@writefile{lot}{\contentsline {table}{\numberline {3.8}{\ignorespaces GAP scores on the candidate ranking task on LS-SE07 for different part-of-speech categories. \relax }}{46}{table.caption.32}\protected@file@percent }
\newlabel{nvaa}{{3.8}{46}{GAP scores on the candidate ranking task on LS-SE07 for different part-of-speech categories. \relax }{table.caption.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Qualitative analysis}{46}{section.3.5}\protected@file@percent }
\newlabel{embindepth}{{3.5}{46}{Qualitative analysis}{section.3.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.9}{\ignorespaces  Examples of word substitution rankings and respective GAP scores. The gold rank includes substitution words and annotators' votes. The models are word embeddings with context (SGE), MSSG \citep  {neelakantan2014efficient}, and our topic-sensitive model (HTLE). Target words in the contexts and correct words in the rankings are bold.\relax }}{47}{table.caption.33}\protected@file@percent }
\newlabel{lsexample}{{3.9}{47}{Examples of word substitution rankings and respective GAP scores. The gold rank includes substitution words and annotators' votes. The models are word embeddings with context (SGE), MSSG \citep {neelakantan2014efficient}, and our topic-sensitive model (HTLE). Target words in the contexts and correct words in the rankings are bold.\relax }{table.caption.33}{}}
\@writefile{brf}{\backcite{neelakantan2014efficient}{{47}{3.9}{table.caption.33}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Visualizing a subset of word-topic pairs using t-SNE to showcase topic assignment separations. Colors distinguish topics. We observe that words that are labeled the same topics end up in the same clusters. \relax }}{48}{figure.caption.34}\protected@file@percent }
\newlabel{vecvis}{{3.5}{48}{Visualizing a subset of word-topic pairs using t-SNE to showcase topic assignment separations. Colors distinguish topics. We observe that words that are labeled the same topics end up in the same clusters. \relax }{figure.caption.34}{}}
\citation{neelakantan2014efficient}
\citation{7b54165e73a3424b8820136bcf61ca89}
\BKM@entry{id=40,dest={73656374696F6E2E332E36},srcline={797}}{436F6E636C7573696F6E}
\@writefile{brf}{\backcite{neelakantan2014efficient}{{49}{3.5}{figure.caption.34}}}
\@writefile{brf}{\backcite{7b54165e73a3424b8820136bcf61ca89}{{49}{3.5}{figure.caption.34}}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Conclusion}{49}{section.3.6}\protected@file@percent }
\newlabel{embconc}{{3.6}{49}{Conclusion}{section.3.6}{}}
\acronymused{rq:topic1}
\acronymused{rq:topic2}
\citation{neelakantan2014efficient}
\citation{mccarthy2007semeval}
\citation{peters-etal-2018-deep,radford2019language,devlin-etal-2019-bert}
\citation{DBLP:journals/corr/abs-1807-07279}
\citation{hurtado-bodell-etal-2019-interpretable}
\acronymused{rq:topic3}
\@writefile{brf}{\backcite{neelakantan2014efficient}{{50}{{{\textbf  {RQ1.3 }}}}{Item.34}}}
\@writefile{brf}{\backcite{mccarthy2007semeval}{{50}{{{\textbf  {RQ1.3 }}}}{Item.34}}}
\@writefile{toc}{\contentsline {paragraph}{Research Question 1:}{50}{paragraph*.35}\protected@file@percent }
\acronymused{rq:topic}
\@writefile{brf}{\backcite{peters-etal-2018-deep}{{50}{3.6}{paragraph*.35}}}
\@writefile{brf}{\backcite{radford2019language}{{50}{3.6}{paragraph*.35}}}
\@writefile{brf}{\backcite{devlin-etal-2019-bert}{{50}{3.6}{paragraph*.35}}}
\@writefile{brf}{\backcite{DBLP:journals/corr/abs-1807-07279}{{51}{3.6}{paragraph*.35}}}
\@writefile{brf}{\backcite{hurtado-bodell-etal-2019-interpretable}{{51}{3.6}{paragraph*.35}}}
\BKM@entry{id=41,dest={636861707465722E34},srcline={3}}{44617461204175676D656E746174696F6E20666F72205261726520576F726473}
\BKM@entry{id=42,dest={73656374696F6E2E342E31},srcline={6}}{496E74726F64756374696F6E20616E64207265736561726368207175657374696F6E73}
\citation{zoph-EtAl:2016:EMNLP2016,koehn2017six,gu-etal-2018-meta,ngo-etal-2019-overcoming}
\citation{ostling2017neural}
\citation{Melamed98manualannotation}
\citation{sennrich-haddow-birch:2016:P16-11,wang-etal-2018-switchout}
\citation{NIPS2012_4824,DBLP:conf/bmvc/ChatfieldSVZ14}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Data Augmentation for Rare Words}{53}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:research-02}{{4}{53}{Data Augmentation for Rare Words}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction and research questions}{53}{section.4.1}\protected@file@percent }
\@writefile{brf}{\backcite{zoph-EtAl:2016:EMNLP2016}{{53}{4.1}{section.4.1}}}
\@writefile{brf}{\backcite{koehn2017six}{{53}{4.1}{section.4.1}}}
\@writefile{brf}{\backcite{gu-etal-2018-meta}{{53}{4.1}{section.4.1}}}
\@writefile{brf}{\backcite{ngo-etal-2019-overcoming}{{53}{4.1}{section.4.1}}}
\@writefile{brf}{\backcite{ostling2017neural}{{53}{4.1}{section.4.1}}}
\@writefile{brf}{\backcite{Melamed98manualannotation}{{53}{4.1}{section.4.1}}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{53}{4.1}{section.4.1}}}
\@writefile{brf}{\backcite{wang-etal-2018-switchout}{{53}{4.1}{section.4.1}}}
\@writefile{brf}{\backcite{NIPS2012_4824}{{53}{4.1}{section.4.1}}}
\@writefile{brf}{\backcite{DBLP:conf/bmvc/ChatfieldSVZ14}{{53}{4.1}{section.4.1}}}
\citation{Halevy:2009:UED:1525642.1525689}
\citation{Marton:2009:ISM:1699510.1699560,duong-EtAl:2015:EMNLP}
\citation{sennrich-haddow-birch:2016:P16-12}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Top: flip and crop, two label-preserving data augmentation techniques in computer vision. Bottom: Altering one sentence in a parallel corpus requires changing its translation.\relax }}{54}{figure.caption.36}\protected@file@percent }
\newlabel{augfig}{{4.1}{54}{Top: flip and crop, two label-preserving data augmentation techniques in computer vision. Bottom: Altering one sentence in a parallel corpus requires changing its translation.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {paragraph}{Research Question 2:}{54}{paragraph*.37}\protected@file@percent }
\acronymused{rq:tdabt}
\@writefile{brf}{\backcite{Halevy:2009:UED:1525642.1525689}{{54}{4.1}{paragraph*.37}}}
\@writefile{brf}{\backcite{Marton:2009:ISM:1699510.1699560}{{54}{4.1}{paragraph*.37}}}
\@writefile{brf}{\backcite{duong-EtAl:2015:EMNLP}{{54}{4.1}{paragraph*.37}}}
\acronymused{rq:tda1}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-12}{{54}{{{\textbf  {RQ2.1 }}}}{Item.35}}}
\BKM@entry{id=43,dest={73656374696F6E2E342E32},srcline={85}}{50726576696F757320776F726B}
\BKM@entry{id=44,dest={73756273656374696F6E2E342E322E31},srcline={92}}{44617461206175676D656E746174696F6E20696E20636F6D707574657220766973696F6E}
\citation{Halevy:2009:UED:1525642.1525689}
\citation{NIPS2012_4824,huang2018gpipe,cubuk2019autoaugment}
\citation{kynkaanniemi2019improved,karras2019style}
\citation{singh2018sniper,liu2016ssd}
\citation{inoue2018data}
\acronymused{rq:tda2}
\@writefile{toc}{\contentsline {paragraph}{Organization.}{55}{paragraph*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Previous work}{55}{section.4.2}\protected@file@percent }
\newlabel{tda:background}{{4.2}{55}{Previous work}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Data augmentation in computer vision}{55}{subsection.4.2.1}\protected@file@percent }
\@writefile{brf}{\backcite{Halevy:2009:UED:1525642.1525689}{{55}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{NIPS2012_4824}{{55}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{huang2018gpipe}{{55}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{cubuk2019autoaugment}{{55}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{kynkaanniemi2019improved}{{55}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{karras2019style}{{55}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{singh2018sniper}{{55}{4.2.1}{subsection.4.2.1}}}
\@writefile{brf}{\backcite{liu2016ssd}{{55}{4.2.1}{subsection.4.2.1}}}
\BKM@entry{id=45,dest={73756273656374696F6E2E342E322E32},srcline={132}}{4C6F772D5265736F75726365207472616E736C6174696F6E}
\citation{koehn2017six}
\citation{sennrich-zhang-2019-revisiting}
\citation{sutskever2014sequence}
\citation{DBLP:journals/corr/BahdanauCB14}
\citation{sennrich-haddow-birch:2016:P16-11}
\citation{abdulmumin2020using}
\citation{currey2017copied}
\citation{costa-jussa-fonollosa-2016-character,sennrich-haddow-birch:2016:P16-12}
\citation{ostling2017neural}
\citation{lee-etal-2017-fully}
\citation{cherry-etal-2018-revisiting}
\@writefile{brf}{\backcite{inoue2018data}{{56}{4.2.1}{subsection.4.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Low-Resource translation}{56}{subsection.4.2.2}\protected@file@percent }
\@writefile{brf}{\backcite{koehn2017six}{{56}{4.2.2}{subsection.4.2.2}}}
\@writefile{brf}{\backcite{sennrich-zhang-2019-revisiting}{{56}{4.2.2}{subsection.4.2.2}}}
\@writefile{brf}{\backcite{sutskever2014sequence}{{56}{4.2.2}{subsection.4.2.2}}}
\@writefile{brf}{\backcite{DBLP:journals/corr/BahdanauCB14}{{56}{4.2.2}{subsection.4.2.2}}}
\@writefile{toc}{\contentsline {paragraph}{Leveraging monolingual data}{56}{paragraph*.39}\protected@file@percent }
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{56}{4.2.2}{paragraph*.39}}}
\@writefile{brf}{\backcite{abdulmumin2020using}{{56}{4.2.2}{paragraph*.39}}}
\@writefile{brf}{\backcite{currey2017copied}{{56}{4.2.2}{paragraph*.39}}}
\citation{zoph-EtAl:2016:EMNLP2016}
\citation{gu2018universal}
\citation{firat-etal-2016-multi,johnson-etal-2017-googles,blackwood-etal-2018-multilingual,aharoni-etal-2019-massively}
\citation{johnson-etal-2017-googles}
\citation{yang-etal-2018-unsupervised,artetxe-etal-2018-unsupervised,artetxe2018iclr,artetxe-etal-2019-effective}
\citation{lample2018unsupervised}
\citation{lample-etal-2018-phrase}
\citation{sun-etal-2020-knowledge}
\@writefile{toc}{\contentsline {paragraph}{Re-designing the model}{57}{paragraph*.40}\protected@file@percent }
\@writefile{brf}{\backcite{costa-jussa-fonollosa-2016-character}{{57}{4.2.2}{paragraph*.40}}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-12}{{57}{4.2.2}{paragraph*.40}}}
\@writefile{brf}{\backcite{ostling2017neural}{{57}{4.2.2}{paragraph*.40}}}
\@writefile{brf}{\backcite{lee-etal-2017-fully}{{57}{4.2.2}{paragraph*.40}}}
\@writefile{brf}{\backcite{cherry-etal-2018-revisiting}{{57}{4.2.2}{paragraph*.40}}}
\@writefile{toc}{\contentsline {paragraph}{Cross-lingual transfer learning}{57}{paragraph*.41}\protected@file@percent }
\@writefile{brf}{\backcite{zoph-EtAl:2016:EMNLP2016}{{57}{4.2.2}{paragraph*.41}}}
\@writefile{brf}{\backcite{gu2018universal}{{57}{4.2.2}{paragraph*.41}}}
\@writefile{brf}{\backcite{firat-etal-2016-multi}{{57}{4.2.2}{paragraph*.41}}}
\@writefile{brf}{\backcite{johnson-etal-2017-googles}{{57}{4.2.2}{paragraph*.41}}}
\@writefile{brf}{\backcite{blackwood-etal-2018-multilingual}{{57}{4.2.2}{paragraph*.41}}}
\@writefile{brf}{\backcite{aharoni-etal-2019-massively}{{57}{4.2.2}{paragraph*.41}}}
\@writefile{brf}{\backcite{johnson-etal-2017-googles}{{57}{4.2.2}{paragraph*.41}}}
\@writefile{toc}{\contentsline {paragraph}{Unsupervised learning}{57}{paragraph*.42}\protected@file@percent }
\@writefile{brf}{\backcite{yang-etal-2018-unsupervised}{{57}{4.2.2}{paragraph*.42}}}
\@writefile{brf}{\backcite{artetxe-etal-2018-unsupervised}{{57}{4.2.2}{paragraph*.42}}}
\@writefile{brf}{\backcite{artetxe2018iclr}{{57}{4.2.2}{paragraph*.42}}}
\@writefile{brf}{\backcite{artetxe-etal-2019-effective}{{57}{4.2.2}{paragraph*.42}}}
\@writefile{brf}{\backcite{lample2018unsupervised}{{57}{4.2.2}{paragraph*.42}}}
\@writefile{brf}{\backcite{lample-etal-2018-phrase}{{57}{4.2.2}{paragraph*.42}}}
\BKM@entry{id=46,dest={73656374696F6E2E342E33},srcline={183}}{44617461206175676D656E746174696F6E20666F72207261726520776F726473}
\@writefile{brf}{\backcite{sun-etal-2020-knowledge}{{58}{4.2.2}{paragraph*.42}}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Data augmentation for rare words}{58}{section.4.3}\protected@file@percent }
\newlabel{tda:model}{{4.3}{58}{Data augmentation for rare words}{section.4.3}{}}
\citation{dyer-chahuneau-smith:2013:NAACL-HLT}
\citation{koehn-etal-2007-moses}
\@writefile{toc}{\contentsline {paragraph}{Targeted words selection:}{59}{paragraph*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rare word substitution:}{59}{paragraph*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Translation selection:}{59}{paragraph*.45}\protected@file@percent }
\@writefile{brf}{\backcite{koehn-etal-2007-moses}{{59}{4.3}{paragraph*.45}}}
\@writefile{toc}{\contentsline {paragraph}{Sampling:}{59}{paragraph*.46}\protected@file@percent }
\@writefile{brf}{\backcite{dyer-chahuneau-smith:2013:NAACL-HLT}{{59}{1}{Hfootnote.4}}}
\@writefile{toc}{\contentsline {paragraph}{Augmentation:}{60}{paragraph*.47}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A visual representation of our proposed mechanism for generating new sentence pairs.\relax }}{60}{figure.caption.48}\protected@file@percent }
\newlabel{tdaposter}{{4.2}{60}{A visual representation of our proposed mechanism for generating new sentence pairs.\relax }{figure.caption.48}{}}
\BKM@entry{id=47,dest={73656374696F6E2E342E34},srcline={345}}{4461746120616E64206578706572696D656E74616C207365747570}
\citation{bojar-EtAl:2016:WMT1}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Examples of augmented data with highlighted \relax $\@@underline {\hbox {Original}}\mathsurround \z@ $\relax  words and \textbf  {substituted} words.  \relax }}{61}{table.caption.49}\protected@file@percent }
\newlabel{examples}{{4.1}{61}{Examples of augmented data with highlighted \underline {Original} words and \textbf {substituted} words.  \relax }{table.caption.49}{}}
\citation{sennrich-haddow-birch:2016:P16-12}
\citation{sennrich-haddow-birch:2016:P16-11}
\citation{sennrich-haddow-birch:2016:P16-11}
\citation{sennrich-haddow-birch:2016:P16-11}
\citation{sennrich-haddow-birch:2016:P16-11}
\citation{sennrich-haddow-birch:2016:P16-11}
\citation{Papineni2001}
\BKM@entry{id=48,dest={73656374696F6E2E342E35},srcline={438}}{526573756C7473}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Data and experimental setup}{62}{section.4.4}\protected@file@percent }
\newlabel{tda:exp}{{4.4}{62}{Data and experimental setup}{section.4.4}{}}
\@writefile{brf}{\backcite{bojar-EtAl:2016:WMT1}{{62}{4.4}{section.4.4}}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-12}{{62}{4.4}{section.4.4}}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{62}{4.4}{table.caption.51}}}
\@writefile{brf}{\backcite{Papineni2001}{{62}{4.4}{table.caption.51}}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Results}{62}{section.4.5}\protected@file@percent }
\newlabel{tda:results}{{4.5}{62}{Results}{section.4.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces  Translation performance (BLEU) on German-English WMT test sets (2014, 2015, and 2016) in a simulated low-resource setting. Back-translation refers to the work of\relax \nobreakspace  {}\citet  {sennrich-haddow-birch:2016:P16-11}. Statistically significant improvements are marked $^\blacktriangleup $ at the $p < .01$ and $^\smalltriangleup $ at the $p < .05$ level, with the first superscript referring to baseline and the second to back-translation$_{1:1}$.\relax }}{63}{table.caption.50}\protected@file@percent }
\newlabel{bleuTBdeen}{{4.2}{63}{Translation performance (BLEU) on German-English WMT test sets (2014, 2015, and 2016) in a simulated low-resource setting. Back-translation refers to the work of~\citet {sennrich-haddow-birch:2016:P16-11}. Statistically significant improvements are marked $^\blacktriangleup $ at the $p < .01$ and $^\smalltriangleup $ at the $p < .05$ level, with the first superscript referring to baseline and the second to back-translation$_{1:1}$.\relax }{table.caption.50}{}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{63}{4.2}{table.caption.50}}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces  Translation performance (BLEU) on English-German WMT test sets (2014, 2015, and 2016) in a simulated low-resource setting. Back-translation refers to the work of\relax \nobreakspace  {}\citet  {sennrich-haddow-birch:2016:P16-11}. Statistically significant improvements are marked $^\blacktriangleup $ at the $p < .01$ and $^\smalltriangleup $ at the $p < .05$ level, with the first superscript referring to baseline and the second to back-translation$_{1:1}$.\relax }}{63}{table.caption.51}\protected@file@percent }
\newlabel{bleuTBende}{{4.3}{63}{Translation performance (BLEU) on English-German WMT test sets (2014, 2015, and 2016) in a simulated low-resource setting. Back-translation refers to the work of~\citet {sennrich-haddow-birch:2016:P16-11}. Statistically significant improvements are marked $^\blacktriangleup $ at the $p < .01$ and $^\smalltriangleup $ at the $p < .05$ level, with the first superscript referring to baseline and the second to back-translation$_{1:1}$.\relax }{table.caption.51}{}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{63}{4.3}{table.caption.51}}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Average length of German$\rightarrow $English translation systems, along with the average length of human reference translations (bottom line). Predominantly, we favour longer translations that are close to human reference translations, i.e., models with higher \% Ref ratio. \relax }}{64}{table.caption.52}\protected@file@percent }
\newlabel{sentlen1}{{4.4}{64}{Average length of German$\rightarrow $English translation systems, along with the average length of human reference translations (bottom line). Predominantly, we favour longer translations that are close to human reference translations, i.e., models with higher \% Ref ratio. \relax }{table.caption.52}{}}
\BKM@entry{id=49,dest={73656374696F6E2E342E36},srcline={576}}{4675727468657220616E616C79736973}
\BKM@entry{id=50,dest={73756273656374696F6E2E342E362E31},srcline={624}}{54617267657420776F726473}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Average length of English$\rightarrow $German translation systems, along with the average length of human reference translations (bottom line). Predominantly, we favour longer translations that are close to human reference translations, i.e., models with higher \% Ref ratio. \relax }}{65}{table.caption.53}\protected@file@percent }
\newlabel{sentlen2}{{4.5}{65}{Average length of English$\rightarrow $German translation systems, along with the average length of human reference translations (bottom line). Predominantly, we favour longer translations that are close to human reference translations, i.e., models with higher \% Ref ratio. \relax }{table.caption.53}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Further analysis}{65}{section.4.6}\protected@file@percent }
\newlabel{tda:analysis}{{4.6}{65}{Further analysis}{section.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Target words}{65}{subsection.4.6.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces An example from WMT14 illustrating the effect of augmenting rare words on generation at test time. The translation of the baseline does not include the rare word \textit  {centimetres}, however, the translation of our TDA model generates the rare word and produces a more fluent sentence. Instances of the augmentation of the word \textit  {centimetres} in training data are also provided. \relax }}{66}{table.caption.55}\protected@file@percent }
\newlabel{transex}{{4.6}{66}{An example from WMT14 illustrating the effect of augmenting rare words on generation at test time. The translation of the baseline does not include the rare word \textit {centimetres}, however, the translation of our TDA model generates the rare word and produces a more fluent sentence. Instances of the augmentation of the word \textit {centimetres} in training data are also provided. \relax }{table.caption.55}{}}
\BKM@entry{id=51,dest={73756273656374696F6E2E342E362E32},srcline={640}}{536F7572636520776F726473}
\BKM@entry{id=52,dest={73756273656374696F6E2E342E362E33},srcline={646}}{4E65676174697665206578616D706C6573}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Effect of TDA on the number of unique rare words generated during De$\rightarrow $En translation. $V_R$ is the set of rare words targeted by TDA\textsubscript  {$_{r\ge 1}$} and $V_{ref}$ the reference translation vocabulary.\relax }}{67}{figure.caption.54}\protected@file@percent }
\newlabel{rarewordfreqs}{{4.3}{67}{Effect of TDA on the number of unique rare words generated during De$\rightarrow $En translation. $V_R$ is the set of rare words targeted by TDA\textsubscript {$_{r\ge 1}$} and $V_{ref}$ the reference translation vocabulary.\relax }{figure.caption.54}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Source words}{67}{subsection.4.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Negative examples}{67}{subsection.4.6.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Examples of incorrectly augmented data with highlighted \relax $\@@underline {\hbox {Original}}\mathsurround \z@ $\relax  words and \textbf  {substituted} words. \relax }}{68}{table.caption.56}\protected@file@percent }
\newlabel{tdaexampleswrong}{{4.7}{68}{Examples of incorrectly augmented data with highlighted \underline {Original} words and \textbf {substituted} words. \relax }{table.caption.56}{}}
\BKM@entry{id=53,dest={73756273656374696F6E2E342E362E34},srcline={694}}{576F7264207365676D656E746174696F6E}
\citation{sennrich-haddow-birch:2016:P16-12}
\citation{ngo-etal-2019-overcoming}
\BKM@entry{id=54,dest={73656374696F6E2E342E37},srcline={703}}{4D65616E696E672D50726573657276696E67206175676D656E746174696F6E}
\citation{ganitkevitch-etal-2013-ppdb}
\citation{miller1995wordnet}
\citation{hamp-feldweg-1997-germanet,HENRICH10.264}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.4}Word segmentation}{69}{subsection.4.6.4}\protected@file@percent }
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-12}{{69}{4.6.4}{subsection.4.6.4}}}
\@writefile{brf}{\backcite{ngo-etal-2019-overcoming}{{69}{4.6.4}{subsection.4.6.4}}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Meaning-Preserving augmentation}{69}{section.4.7}\protected@file@percent }
\newlabel{tda:semantic}{{4.7}{69}{Meaning-Preserving augmentation}{section.4.7}{}}
\@writefile{toc}{\contentsline {subparagraph}{PPDB}{69}{subparagraph*.57}\protected@file@percent }
\@writefile{brf}{\backcite{ganitkevitch-etal-2013-ppdb}{{69}{4.7}{subparagraph*.57}}}
\@writefile{toc}{\contentsline {subparagraph}{Wordnet}{69}{subparagraph*.58}\protected@file@percent }
\@writefile{brf}{\backcite{miller1995wordnet}{{69}{4.7}{subparagraph*.58}}}
\citation{mikolov2013distributed}
\@writefile{brf}{\backcite{hamp-feldweg-1997-germanet}{{70}{4.7}{subparagraph*.58}}}
\@writefile{brf}{\backcite{HENRICH10.264}{{70}{4.7}{subparagraph*.58}}}
\@writefile{toc}{\contentsline {subparagraph}{CBOW}{70}{subparagraph*.59}\protected@file@percent }
\@writefile{brf}{\backcite{mikolov2013distributed}{{70}{4.7}{subparagraph*.59}}}
\@writefile{toc}{\contentsline {subparagraph}{HTLE}{70}{subparagraph*.60}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces Examples of paraphrase modification. The out-of-vocabulary word \textit  {fateful} in the source sentence is substituted with synonyms obtained from different lexicon resources. \relax }}{70}{table.caption.61}\protected@file@percent }
\newlabel{augwnexample}{{4.8}{70}{Examples of paraphrase modification. The out-of-vocabulary word \textit {fateful} in the source sentence is substituted with synonyms obtained from different lexicon resources. \relax }{table.caption.61}{}}
\BKM@entry{id=55,dest={73656374696F6E2E342E38},srcline={825}}{436F6E636C7573696F6E}
\@writefile{lot}{\contentsline {table}{\numberline {4.9}{\ignorespaces Translation performance (BLEU) on German-English WMT news test sets (2014, 2015, and 2016). \texttt  {OOV} signifies out-of-vocabulary words. \texttt  {rare} words are selected with the frequency threshold of 100. \relax }}{71}{table.caption.62}\protected@file@percent }
\newlabel{semdeen}{{4.9}{71}{Translation performance (BLEU) on German-English WMT news test sets (2014, 2015, and 2016). \texttt {OOV} signifies out-of-vocabulary words. \texttt {rare} words are selected with the frequency threshold of 100. \relax }{table.caption.62}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.10}{\ignorespaces Translation performance (BLEU) on English-German WMT news test sets (2014, 2015, and 2016). \texttt  {OOV} signifies out-of-vocabulary words. \texttt  {rare} words are selected with the frequency threshold of 100. \relax }}{71}{table.caption.63}\protected@file@percent }
\newlabel{semende}{{4.10}{71}{Translation performance (BLEU) on English-German WMT news test sets (2014, 2015, and 2016). \texttt {OOV} signifies out-of-vocabulary words. \texttt {rare} words are selected with the frequency threshold of 100. \relax }{table.caption.63}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.11}{\ignorespaces The impact of paraphrase augmentation on the generation of \texttt  {unk} tokens in the translation output. Reductions are computed in comparison with the baseline model. Lower number of \texttt  {unk}s is better.  \relax }}{72}{table.caption.64}\protected@file@percent }
\newlabel{unkaveaug}{{4.11}{72}{The impact of paraphrase augmentation on the generation of \texttt {unk} tokens in the translation output. Reductions are computed in comparison with the baseline model. Lower number of \texttt {unk}s is better.  \relax }{table.caption.64}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Conclusion}{72}{section.4.8}\protected@file@percent }
\newlabel{tda:conc}{{4.8}{72}{Conclusion}{section.4.8}{}}
\acronymused{rq:tda1}
\acronymused{rq:tda2}
\@writefile{toc}{\contentsline {paragraph}{Research Question 2:}{73}{paragraph*.65}\protected@file@percent }
\acronymused{rq:tdabt}
\BKM@entry{id=56,dest={636861707465722E35},srcline={2}}{44617461204175676D656E746174696F6E204261736564206F6E204D6F64656C204661696C757265}
\BKM@entry{id=57,dest={73656374696F6E2E352E31},srcline={6}}{496E74726F64756374696F6E20616E64207265736561726368207175657374696F6E73}
\citation{Lambert:2011:ITM:2132960.2132997}
\citation{sennrich-haddow-birch:2016:P16-11}
\citation{2017arXiv170800726S,2017arXiv170704499G,2017arXiv171107893H}
\citation{sennrich-haddow-birch:2016:P16-11}
\citation{D11-1033,vanderwees-bisazza-monz:2017:EMNLP2017}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Data Augmentation Based on Model Failure}{75}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:research-03}{{5}{75}{Data Augmentation Based on Model Failure}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction and research questions}{75}{section.5.1}\protected@file@percent }
\@writefile{brf}{\backcite{Lambert:2011:ITM:2132960.2132997}{{75}{5.1}{section.5.1}}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{75}{5.1}{section.5.1}}}
\@writefile{brf}{\backcite{2017arXiv170800726S}{{75}{5.1}{section.5.1}}}
\@writefile{brf}{\backcite{2017arXiv170704499G}{{75}{5.1}{section.5.1}}}
\@writefile{brf}{\backcite{2017arXiv171107893H}{{75}{5.1}{section.5.1}}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{75}{5.1}{section.5.1}}}
\@writefile{brf}{\backcite{D11-1033}{{75}{5.1}{section.5.1}}}
\@writefile{brf}{\backcite{vanderwees-bisazza-monz:2017:EMNLP2017}{{75}{5.1}{section.5.1}}}
\@writefile{toc}{\contentsline {paragraph}{Research Question 2:}{76}{paragraph*.66}\protected@file@percent }
\acronymused{rq:tdabt}
\acronymused{rq:bt1}
\acronymused{rq:bt2}
\@writefile{toc}{\contentsline {paragraph}{Organization.}{76}{paragraph*.67}\protected@file@percent }
\BKM@entry{id=58,dest={73656374696F6E2E352E32},srcline={76}}{52656C6174656420776F726B}
\BKM@entry{id=59,dest={73756273656374696F6E2E352E322E31},srcline={80}}{446174612073656C656374696F6E20696E206D616368696E65207472616E736C6174696F6E}
\citation{moore-lewis-2010-intelligent,wang-etal-2013-edit}
\citation{D11-1033}
\citation{silva-etal-2018-extracting,wang-etal-2019-dynamically}
\citation{vanderwees-bisazza-monz:2017:EMNLP2017}
\citation{D11-1033}
\citation{2019arXiv190607808P}
\citation{DBLP:journals/corr/abs-1909-03750}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Related work}{77}{section.5.2}\protected@file@percent }
\newlabel{btrelated}{{5.2}{77}{Related work}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Data selection in machine translation}{77}{subsection.5.2.1}\protected@file@percent }
\@writefile{brf}{\backcite{moore-lewis-2010-intelligent}{{77}{5.2.1}{subsection.5.2.1}}}
\@writefile{brf}{\backcite{wang-etal-2013-edit}{{77}{5.2.1}{subsection.5.2.1}}}
\@writefile{brf}{\backcite{D11-1033}{{77}{5.2.1}{subsection.5.2.1}}}
\@writefile{brf}{\backcite{silva-etal-2018-extracting}{{77}{5.2.1}{subsection.5.2.1}}}
\@writefile{brf}{\backcite{wang-etal-2019-dynamically}{{77}{5.2.1}{subsection.5.2.1}}}
\@writefile{brf}{\backcite{vanderwees-bisazza-monz:2017:EMNLP2017}{{77}{5.2.1}{subsection.5.2.1}}}
\@writefile{brf}{\backcite{D11-1033}{{77}{5.2.1}{subsection.5.2.1}}}
\@writefile{brf}{\backcite{2019arXiv190607808P}{{77}{5.2.1}{subsection.5.2.1}}}
\@writefile{brf}{\backcite{DBLP:journals/corr/abs-1909-03750}{{77}{5.2.1}{subsection.5.2.1}}}
\BKM@entry{id=60,dest={73656374696F6E2E352E33},srcline={101}}{4461746120616E64206578706572696D656E74616C207365747570}
\citation{bojar-EtAl:2017:WMT1}
\citation{2017opennmt}
\citation{sennrich-haddow-birch:2016:P16-12}
\citation{sennrich-haddow-birch:2016:P16-11}
\citation{Papineni2001}
\BKM@entry{id=61,dest={73656374696F6E2E352E34},srcline={126}}{416E616C797A696E67206261636B2D7472616E736C6174696F6E20776974682072616E646F6D2073616D706C696E67}
\BKM@entry{id=62,dest={73756273656374696F6E2E352E342E31},srcline={131}}{496D70616374206F662073796E74686574696320646174612073697A65}
\citation{sennrich-haddow-birch:2016:P16-11}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Data and experimental setup}{78}{section.5.3}\protected@file@percent }
\newlabel{btdata}{{5.3}{78}{Data and experimental setup}{section.5.3}{}}
\@writefile{brf}{\backcite{bojar-EtAl:2017:WMT1}{{78}{5.3}{section.5.3}}}
\@writefile{brf}{\backcite{2017opennmt}{{78}{5.3}{section.5.3}}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-12}{{78}{5.3}{section.5.3}}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{78}{5.3}{section.5.3}}}
\@writefile{brf}{\backcite{Papineni2001}{{78}{5.3}{section.5.3}}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Illustration of three set of experiments analyzing different impacts of synthetic data as additional training data: \textit  {size} (left), \textit  {direction} (middle), and \textit  {quality} (right). \relax }}{78}{figure.caption.68}\protected@file@percent }
\newlabel{btmets}{{5.1}{78}{Illustration of three set of experiments analyzing different impacts of synthetic data as additional training data: \textit {size} (left), \textit {direction} (middle), and \textit {quality} (right). \relax }{figure.caption.68}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Analyzing back-translation with random sampling}{78}{section.5.4}\protected@file@percent }
\newlabel{btbtanalysis}{{5.4}{78}{Analyzing back-translation with random sampling}{section.5.4}{}}
\citation{2018arXiv180406189P}
\citation{edunov-etal-2018-understanding}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Impact of synthetic data size}{79}{subsection.5.4.1}\protected@file@percent }
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{79}{5.4.1}{subsection.5.4.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Training plots for systems with different ratios of (\unhbox \voidb@x \hbox {$real:syn$}) training data, showing perplexity on development set.\relax }}{79}{figure.caption.69}\protected@file@percent }
\newlabel{ppl}{{5.2}{79}{Training plots for systems with different ratios of (\mbox {$real:syn$}) training data, showing perplexity on development set.\relax }{figure.caption.69}{}}
\@writefile{brf}{\backcite{2018arXiv180406189P}{{79}{5.4.1}{figure.caption.69}}}
\@writefile{brf}{\backcite{edunov-etal-2018-understanding}{{79}{5.4.1}{table.caption.70}}}
\BKM@entry{id=63,dest={73756273656374696F6E2E352E342E32},srcline={181}}{496D70616374206F66207472616E736C6174696F6E20646972656374696F6E}
\citation{Lambert:2011:ITM:2132960.2132997}
\citation{zhang-zong-2016-exploiting}
\citation{Lambert:2011:ITM:2132960.2132997}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces  German$\rightarrow $English translation quality ({BLEU}) of systems with different ratios of \textit  {\unhbox \voidb@x \hbox {real:syn}} data.\relax }}{80}{table.caption.70}\protected@file@percent }
\newlabel{bigbigger}{{5.1}{80}{German$\rightarrow $English translation quality ({BLEU}) of systems with different ratios of \textit {\mbox {real:syn}} data.\relax }{table.caption.70}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Impact of translation direction}{80}{subsection.5.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces  English$\rightarrow $German translation quality ({BLEU}) of systems using forward and reverse models for generating synthetic data.\relax }}{80}{table.caption.71}\protected@file@percent }
\newlabel{dir}{{5.2}{80}{English$\rightarrow $German translation quality ({BLEU}) of systems using forward and reverse models for generating synthetic data.\relax }{table.caption.71}{}}
\@writefile{brf}{\backcite{Lambert:2011:ITM:2132960.2132997}{{80}{5.4.2}{table.caption.71}}}
\@writefile{brf}{\backcite{zhang-zong-2016-exploiting}{{80}{5.4.2}{table.caption.71}}}
\BKM@entry{id=64,dest={73756273656374696F6E2E352E342E33},srcline={216}}{496D70616374206F66207175616C697479206F66207468652073796E7468657469632064617461}
\citation{W18-2709}
\citation{sennrich-haddow-birch:2016:P16-11}
\@writefile{brf}{\backcite{Lambert:2011:ITM:2132960.2132997}{{81}{5.4.2}{table.caption.71}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Impact of quality of the synthetic data}{81}{subsection.5.4.3}\protected@file@percent }
\@writefile{brf}{\backcite{W18-2709}{{81}{5.4.3}{subsection.5.4.3}}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces  German$\rightarrow $English translation quality ({BLEU}) of systems using synthetic source and human generated source data.\relax }}{81}{table.caption.72}\protected@file@percent }
\newlabel{groundt}{{5.3}{81}{German$\rightarrow $English translation quality ({BLEU}) of systems using synthetic source and human generated source data.\relax }{table.caption.72}{}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{81}{5.4.3}{table.caption.72}}}
\BKM@entry{id=65,dest={73656374696F6E2E352E35},srcline={250}}{4261636B2D5472616E736C6174696F6E20616E6420746F6B656E2070726564696374696F6E206C6F7373}
\citation{sennrich-haddow-birch:2016:P16-11}
\citation{chen1996empirical}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Back-Translation and token prediction loss}{82}{section.5.5}\protected@file@percent }
\newlabel{bttokpred}{{5.5}{82}{Back-Translation and token prediction loss}{section.5.5}{}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{82}{5.5}{figure.caption.73}}}
\@writefile{brf}{\backcite{chen1996empirical}{{82}{5.5}{figure.caption.73}}}
\BKM@entry{id=66,dest={73656374696F6E2E352E36},srcline={298}}{54617267657465642073616D706C696E67206261736564206F6E206D6F64656C206661696C757265}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Top: Changes in mean prediction loss after re-training with synthetic data sorted by mean prediction loss of the baseline system (x-axis). Decreases and increases in values are marked blue and red, respectively. Bottom: Frequencies (log) of target tokens in the baseline training data. Note that data points in both plots (x-axis) represent \textit  {token} types in the vocabulary.\relax }}{83}{figure.caption.73}\protected@file@percent }
\newlabel{loss}{{5.3}{83}{Top: Changes in mean prediction loss after re-training with synthetic data sorted by mean prediction loss of the baseline system (x-axis). Decreases and increases in values are marked blue and red, respectively. Bottom: Frequencies (log) of target tokens in the baseline training data. Note that data points in both plots (x-axis) represent \textit {token} types in the vocabulary.\relax }{figure.caption.73}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Targeted sampling based on model failure}{83}{section.5.6}\protected@file@percent }
\newlabel{bttarget}{{5.6}{83}{Targeted sampling based on model failure}{section.5.6}{}}
\BKM@entry{id=67,dest={73756273656374696F6E2E352E362E31},srcline={337}}{546F6B656E206672657175656E637920617320612066656174757265206F6620646966666963756C7479}
\BKM@entry{id=68,dest={73756273656374696F6E2E352E362E32},srcline={352}}{546F6B656E7320776974682068696768206D65616E2070726564696374696F6E206C6F73736573}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Sampling for difficult words\relax }}{84}{algorithm.1}\protected@file@percent }
\newlabel{alg1}{{1}{84}{Sampling for difficult words\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Token frequency as a feature of difficulty}{84}{subsection.5.6.1}\protected@file@percent }
\BKM@entry{id=69,dest={73756273656374696F6E2E352E362E33},srcline={374}}{546F6B656E73207769746820736B657765642070726564696374696F6E206C6F73736573}
\BKM@entry{id=70,dest={73756273656374696F6E2E352E362E34},srcline={391}}{50726573657276696E672073616D706C696E6720726174696F206F6620646966666963756C74206F6363757272656E636573}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Tokens with high mean prediction losses}{85}{subsection.5.6.2}\protected@file@percent }
\newlabel{ave}{{5.3}{85}{Tokens with high mean prediction losses}{equation.5.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}Tokens with skewed prediction losses}{85}{subsection.5.6.3}\protected@file@percent }
\BKM@entry{id=71,dest={73756273656374696F6E2E352E362E35},srcline={434}}{526573756C7473}
\citation{sennrich-haddow-birch:2016:P16-11}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.4}Preserving sampling ratio of difficult occurrences}{86}{subsection.5.6.4}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Sampling with ratio preservation\relax }}{86}{algorithm.2}\protected@file@percent }
\newlabel{alg2}{{2}{86}{Sampling with ratio preservation\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.5}Results}{86}{subsection.5.6.5}\protected@file@percent }
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-11}{{86}{5.6.5}{subsection.5.6.5}}}
\BKM@entry{id=72,dest={73656374696F6E2E352E37},srcline={515}}{436F6E746578742D41776172652074617267657465642073616D706C696E67}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Examples of changes in average prediction loss after augmentation. Lower is better. The sizes of dots are proportional to the increase in the number of contexts for each word in the training data. Subword unit boundaries are marked with `@@'. \relax }}{87}{figure.caption.74}\protected@file@percent }
\newlabel{predlossexample}{{5.4}{87}{Examples of changes in average prediction loss after augmentation. Lower is better. The sizes of dots are proportional to the increase in the number of contexts for each word in the training data. Subword unit boundaries are marked with `@@'. \relax }{figure.caption.74}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces  English$\rightarrow $German translation quality ({BLEU}). Experiments marked $^\dagger $ are averaged over 3 runs. \textsc  {Random} is the standard back-translation approach with random sampling. \textsc  {MPL} and \textsc  {Freq} are difficulty criteria based on mean prediction loss and token frequency, respectively. \textsc  {MPL + sPL} is experiments with upsampling tokens with skewed prediction losses. \textsc  {PPLR} preserves the ratio of the distribution of difficult contexts. \relax }}{88}{table.caption.75}\protected@file@percent }
\newlabel{meanresultsende}{{5.4}{88}{English$\rightarrow $German translation quality ({BLEU}). Experiments marked $^\dagger $ are averaged over 3 runs. \textsc {Random} is the standard back-translation approach with random sampling. \textsc {MPL} and \textsc {Freq} are difficulty criteria based on mean prediction loss and token frequency, respectively. \textsc {MPL + sPL} is experiments with upsampling tokens with skewed prediction losses. \textsc {PPLR} preserves the ratio of the distribution of difficult contexts. \relax }{table.caption.75}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces   German$\rightarrow $English translation quality ({BLEU}). Experiments marked $^\dagger $ are averaged over 3 runs. \textsc  {Random} is the standard back-translation approach with random sampling. \textsc  {MPL} and \textsc  {Freq} are difficulty criteria based on mean prediction loss and token frequency, respectively. \textsc  {MPL + sPL} is experiments with upsampling tokens with skewed prediction losses. \textsc  {PPLR} preserves the ratio of the distribution of difficult contexts. \relax }}{88}{table.caption.76}\protected@file@percent }
\newlabel{meanresultsdeen}{{5.5}{88}{German$\rightarrow $English translation quality ({BLEU}). Experiments marked $^\dagger $ are averaged over 3 runs. \textsc {Random} is the standard back-translation approach with random sampling. \textsc {MPL} and \textsc {Freq} are difficulty criteria based on mean prediction loss and token frequency, respectively. \textsc {MPL + sPL} is experiments with upsampling tokens with skewed prediction losses. \textsc {PPLR} preserves the ratio of the distribution of difficult contexts. \relax }{table.caption.76}{}}
\BKM@entry{id=73,dest={73756273656374696F6E2E352E372E31},srcline={566}}{446566696E6974696F6E206F66206C6F63616C20636F6E74657874}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Context-Aware targeted sampling}{89}{section.5.7}\protected@file@percent }
\newlabel{btcontextu}{{5.7}{89}{Context-Aware targeted sampling}{section.5.7}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Sampling with context\relax }}{89}{algorithm.3}\protected@file@percent }
\newlabel{alg3}{{3}{89}{Sampling with context\relax }{algorithm.3}{}}
\newlabel{cont1}{{6}{89}{Sampling with context\relax }{algorithm.3}{}}
\newlabel{cont2}{{8}{89}{Sampling with context\relax }{algorithm.3}{}}
\newlabel{simss}{{9}{89}{Sampling with context\relax }{algorithm.3}{}}
\citation{sennrich-haddow-birch:2016:P16-12}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Definition of local context}{90}{subsection.5.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Neighboring tokens}{90}{paragraph*.77}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sibling tokens}{90}{paragraph*.78}\protected@file@percent }
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-12}{{90}{5.7.1}{paragraph*.78}}}
\BKM@entry{id=74,dest={73756273656374696F6E2E352E372E32},srcline={645}}{53696D696C6172697479206F6620746865206C6F63616C20636F6E7465787473}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces  An example from the synthetic data where the word \textit  {B$\mid $ahr} is incorrectly translated to \textit  {B$\mid $risk}. Subword unit boundaries are marked with `$\mid $'.\relax }}{91}{table.caption.79}\protected@file@percent }
\newlabel{exbpe1}{{5.6}{91}{An example from the synthetic data where the word \textit {B$\mid $ahr} is incorrectly translated to \textit {B$\mid $risk}. Subword unit boundaries are marked with `$\mid $'.\relax }{table.caption.79}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces  Results of context-aware targeted sampling with sibling tokens for the difficult subword unit \textit  {`Stan'}. In this example, the difficult context in which the subword \textit  {`Stan'} has a high prediction loss is the complete word \textit  {`Stanford'} and we sample sentences containing this word.\relax }}{91}{table.caption.80}\protected@file@percent }
\newlabel{exbpe2}{{5.7}{91}{Results of context-aware targeted sampling with sibling tokens for the difficult subword unit \textit {`Stan'}. In this example, the difficult context in which the subword \textit {`Stan'} has a high prediction loss is the complete word \textit {`Stanford'} and we sample sentences containing this word.\relax }{table.caption.80}{}}
\citation{mikolov2013efficient}
\BKM@entry{id=75,dest={73756273656374696F6E2E352E372E33},srcline={695}}{526573756C7473}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}Similarity of the local contexts}{92}{subsection.5.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Matching the local context (Exct)}{92}{paragraph*.81}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Word representations (Sem)}{92}{paragraph*.82}\protected@file@percent }
\@writefile{brf}{\backcite{mikolov2013efficient}{{92}{5.7.2}{paragraph*.82}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.3}Results}{92}{subsection.5.7.3}\protected@file@percent }
\BKM@entry{id=76,dest={73656374696F6E2E352E38},srcline={781}}{5175616C6974617469766520726573756C7473}
\@writefile{lot}{\contentsline {table}{\numberline {5.8}{\ignorespaces  Results of context-aware targeted sampling for the difficult token \textit  {`Rock'} \relax }}{93}{table.caption.83}\protected@file@percent }
\newlabel{context}{{5.8}{93}{Results of context-aware targeted sampling for the difficult token \textit {`Rock'} \relax }{table.caption.83}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Qualitative results}{93}{section.5.8}\protected@file@percent }
\newlabel{btanalysisagain}{{5.8}{93}{Qualitative results}{section.5.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.9}{\ignorespaces  German$\rightarrow $English translation quality ({BLEU}). Experiments marked $^\dagger $ are averaged over 3 runs. \textsc  {Random} is the standard back-translation approach with random sampling. \textsc  {PredLoss} is the contextual prediction loss and \textsc  {MPL} is the average loss. \textit  {token} and \textit  {SubUnit} are context selection definitions from neighboring tokens and subword units, respectively. Note that token includes both subword units and full words. \textit  {Sent} regards the entire sentence as the context. \textit  {Sem} is computing context similarities with token embeddings and \textit  {Exct} is comparing the context tokens.\relax }}{94}{table.caption.84}\protected@file@percent }
\newlabel{contextresultsdeen}{{5.9}{94}{German$\rightarrow $English translation quality ({BLEU}). Experiments marked $^\dagger $ are averaged over 3 runs. \textsc {Random} is the standard back-translation approach with random sampling. \textsc {PredLoss} is the contextual prediction loss and \textsc {MPL} is the average loss. \textit {token} and \textit {SubUnit} are context selection definitions from neighboring tokens and subword units, respectively. Note that token includes both subword units and full words. \textit {Sent} regards the entire sentence as the context. \textit {Sem} is computing context similarities with token embeddings and \textit {Exct} is comparing the context tokens.\relax }{table.caption.84}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.10}{\ignorespaces  English$\rightarrow $German translation quality ({BLEU}). Experiments marked $^\dagger $ are averaged over 3 runs. \textsc  {Random} is the standard back-translation approach with random sampling. \textsc  {PredLoss} is the contextual prediction loss and \textsc  {MPL} is the average loss. \textit  {token} and \textit  {SubUnit} are context selection definitions from neighboring tokens and subword units, respectively. Note that token includes both subword units and full words. \textit  {Sent} denotes the sentence as the context. \textit  {Sem} is computing context similarities with token embeddings and \textit  {Exct} is comparing the context tokens.\relax }}{95}{table.caption.85}\protected@file@percent }
\newlabel{contextresultsende}{{5.10}{95}{English$\rightarrow $German translation quality ({BLEU}). Experiments marked $^\dagger $ are averaged over 3 runs. \textsc {Random} is the standard back-translation approach with random sampling. \textsc {PredLoss} is the contextual prediction loss and \textsc {MPL} is the average loss. \textit {token} and \textit {SubUnit} are context selection definitions from neighboring tokens and subword units, respectively. Note that token includes both subword units and full words. \textit {Sent} denotes the sentence as the context. \textit {Sem} is computing context similarities with token embeddings and \textit {Exct} is comparing the context tokens.\relax }{table.caption.85}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Visualization of token prediction loss (final training epoch) for the subword \textit  {\textbf  {danger@@}} in three different sentences. Darker means the model has less confidence predicting the word. Subword unit boundaries are marked with `@@' \relax }}{96}{figure.caption.86}\protected@file@percent }
\newlabel{loosexamples}{{5.5}{96}{Visualization of token prediction loss (final training epoch) for the subword \textit {\textbf {danger@@}} in three different sentences. Darker means the model has less confidence predicting the word. Subword unit boundaries are marked with `@@' \relax }{figure.caption.86}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Prediction losses of 1000 randomly sampled sentences of the same length (20 tokens) from the training data. Darker means the model has less confidence predicting the word. \relax }}{96}{figure.caption.87}\protected@file@percent }
\newlabel{losepos}{{5.6}{96}{Prediction losses of 1000 randomly sampled sentences of the same length (20 tokens) from the training data. Darker means the model has less confidence predicting the word. \relax }{figure.caption.87}{}}
\BKM@entry{id=77,dest={73656374696F6E2E352E39},srcline={841}}{436F6E636C7573696F6E}
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Conclusion}{97}{section.5.9}\protected@file@percent }
\newlabel{btconc}{{5.9}{97}{Conclusion}{section.5.9}{}}
\acronymused{rq:bt1}
\acronymused{rq:bt2}
\@writefile{toc}{\contentsline {paragraph}{Research Question 2:}{98}{paragraph*.88}\protected@file@percent }
\acronymused{rq:tdabt}
\BKM@entry{id=78,dest={636861707465722E36},srcline={3}}{5472616E736C6174696E67204964696F6D617469632045787072657373696F6E73}
\BKM@entry{id=79,dest={73656374696F6E2E362E31},srcline={6}}{496E74726F64756374696F6E20616E64207265736561726368207175657374696F6E73}
\citation{isabelle2017challenge,bentivogli-EtAl:2016:EMNLP2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Translating Idiomatic Expressions}{99}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:research-04}{{6}{99}{Translating Idiomatic Expressions}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction and research questions}{99}{section.6.1}\protected@file@percent }
\@writefile{brf}{\backcite{isabelle2017challenge}{{99}{6.1}{section.6.1}}}
\@writefile{brf}{\backcite{bentivogli-EtAl:2016:EMNLP2016}{{99}{6.1}{section.6.1}}}
\@writefile{toc}{\contentsline {paragraph}{Research Question 3:}{99}{paragraph*.89}\protected@file@percent }
\acronymused{rq:vol}
\BKM@entry{id=80,dest={73656374696F6E2E362E32},srcline={78}}{4964696F6D617469632065787072657373696F6E73}
\citation{10.2307/416483,doi:10.1093/applin/17.3.326}
\citation{10.2307/416483}
\acronymused{rq:id1}
\acronymused{rq:id2}
\@writefile{toc}{\contentsline {paragraph}{Organization.}{100}{paragraph*.90}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Idiomatic expressions}{100}{section.6.2}\protected@file@percent }
\newlabel{idrel}{{6.2}{100}{Idiomatic expressions}{section.6.2}{}}
\@writefile{brf}{\backcite{10.2307/416483}{{100}{6.2}{section.6.2}}}
\@writefile{brf}{\backcite{doi:10.1093/applin/17.3.326}{{100}{6.2}{section.6.2}}}
\@writefile{brf}{\backcite{10.2307/416483}{{100}{6.2}{section.6.2}}}
\BKM@entry{id=81,dest={73756273656374696F6E2E362E322E31},srcline={88}}{4964696F6D206964656E74696669636174696F6E}
\citation{evert-krenn-2001-methods,evert-kermes-2003-experiments}
\citation{katz-giesbrecht-2006-automatic}
\citation{salehi-cook-2013-predicting}
\citation{salehi-etal-2015-word}
\citation{salton-etal-2016-idiom}
\citation{NIPS2015_5950}
\citation{klyueva-etal-2017-neural}
\BKM@entry{id=82,dest={73756273656374696F6E2E362E322E32},srcline={102}}{4964696F6D207472616E736C6174696F6E}
\citation{Schenk:1986:IRM:991365.991458}
\citation{salton-etal-2014-evaluation,isabelle2017challenge,agrawal-etal-2018-beating}
\citation{wu2016google}
\citation{2017opennmt}
\citation{DBLP:journals/corr/BahdanauCB14}
\citation{luong:2015:EMNLP}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Idiom identification}{101}{subsection.6.2.1}\protected@file@percent }
\@writefile{brf}{\backcite{evert-krenn-2001-methods}{{101}{6.2.1}{subsection.6.2.1}}}
\@writefile{brf}{\backcite{evert-kermes-2003-experiments}{{101}{6.2.1}{subsection.6.2.1}}}
\@writefile{brf}{\backcite{katz-giesbrecht-2006-automatic}{{101}{6.2.1}{subsection.6.2.1}}}
\@writefile{brf}{\backcite{salehi-cook-2013-predicting}{{101}{6.2.1}{subsection.6.2.1}}}
\@writefile{brf}{\backcite{salehi-etal-2015-word}{{101}{6.2.1}{subsection.6.2.1}}}
\@writefile{brf}{\backcite{salton-etal-2016-idiom}{{101}{6.2.1}{subsection.6.2.1}}}
\@writefile{brf}{\backcite{NIPS2015_5950}{{101}{6.2.1}{subsection.6.2.1}}}
\@writefile{brf}{\backcite{klyueva-etal-2017-neural}{{101}{6.2.1}{subsection.6.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Idiom translation}{101}{subsection.6.2.2}\protected@file@percent }
\@writefile{brf}{\backcite{Schenk:1986:IRM:991365.991458}{{101}{6.2.2}{subsection.6.2.2}}}
\@writefile{brf}{\backcite{salton-etal-2014-evaluation}{{101}{6.2.2}{subsection.6.2.2}}}
\@writefile{brf}{\backcite{isabelle2017challenge}{{101}{6.2.2}{subsection.6.2.2}}}
\@writefile{brf}{\backcite{agrawal-etal-2018-beating}{{101}{6.2.2}{subsection.6.2.2}}}
\@writefile{brf}{\backcite{wu2016google}{{101}{6.2.2}{table.caption.91}}}
\citation{muzny2013automatic,markantonatou2017proceedings}
\citation{salton-ross-kelleher:2014:HyTra}
\citation{isabelle2017challenge}
\citation{shao-etal-2018-evaluating}
\citation{moussallem-etal-2018-lidioms}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Example of an idiomatic phrase in German and its translation. We compare the output of state-of-the-art commercial models (DeepL and GoogleNMT), as well as our trained model (based on OpenNMT). In translating a sentence containing this idiomatic phrase, we notice that none capture the idiom translation correctly. \relax }}{102}{table.caption.91}\protected@file@percent }
\newlabel{examplesinc}{{6.1}{102}{Example of an idiomatic phrase in German and its translation. We compare the output of state-of-the-art commercial models (DeepL and GoogleNMT), as well as our trained model (based on OpenNMT). In translating a sentence containing this idiomatic phrase, we notice that none capture the idiom translation correctly. \relax }{table.caption.91}{}}
\@writefile{brf}{\backcite{2017opennmt}{{102}{6.2.2}{table.caption.91}}}
\@writefile{brf}{\backcite{DBLP:journals/corr/BahdanauCB14}{{102}{6.2.2}{table.caption.91}}}
\@writefile{brf}{\backcite{luong:2015:EMNLP}{{102}{6.2.2}{table.caption.91}}}
\@writefile{brf}{\backcite{muzny2013automatic}{{102}{6.2.2}{table.caption.91}}}
\@writefile{brf}{\backcite{markantonatou2017proceedings}{{102}{6.2.2}{table.caption.91}}}
\@writefile{brf}{\backcite{salton-ross-kelleher:2014:HyTra}{{102}{6.2.2}{table.caption.91}}}
\@writefile{brf}{\backcite{isabelle2017challenge}{{102}{6.2.2}{table.caption.91}}}
\@writefile{brf}{\backcite{shao-etal-2018-evaluating}{{102}{6.2.2}{table.caption.91}}}
\@writefile{brf}{\backcite{moussallem-etal-2018-lidioms}{{102}{6.2.2}{table.caption.91}}}
\BKM@entry{id=83,dest={73656374696F6E2E362E33},srcline={153}}{4461746120636F6C6C656374696F6E}
\citation{bojar-EtAl:2017:WMT1}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Data collection}{103}{section.6.3}\protected@file@percent }
\newlabel{idiomdata}{{6.3}{103}{Data collection}{section.6.3}{}}
\@writefile{brf}{\backcite{bojar-EtAl:2017:WMT1}{{103}{6.3}{section.6.3}}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Two examples displaying different constraints of matching an idiom phrase with occurrences in the sentence. \relax }}{103}{table.caption.92}\protected@file@percent }
\newlabel{two}{{6.2}{103}{Two examples displaying different constraints of matching an idiom phrase with occurrences in the sentence. \relax }{table.caption.92}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The process of data collection and construction of the test set containing only sentence pairs with idiomatic phrases.\relax }}{104}{figure.caption.93}\protected@file@percent }
\newlabel{augfig2}{{6.1}{104}{The process of data collection and construction of the test set containing only sentence pairs with idiomatic phrases.\relax }{figure.caption.93}{}}
\BKM@entry{id=84,dest={73656374696F6E2E362E34},srcline={291}}{5472616E736C6174696F6E206578706572696D656E7473}
\citation{sennrich-etal-2016-controlling}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Statistics of the constructed German and English idiom translation data sets. \relax }}{105}{table.caption.94}\protected@file@percent }
\newlabel{stats}{{6.3}{105}{Statistics of the constructed German and English idiom translation data sets. \relax }{table.caption.94}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Translation experiments}{105}{section.6.4}\protected@file@percent }
\newlabel{idexperiments}{{6.4}{105}{Translation experiments}{section.6.4}{}}
\@writefile{brf}{\backcite{sennrich-etal-2016-controlling}{{105}{6.4}{section.6.4}}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Examples from the German idiom translation test set. \relax }}{106}{table.caption.95}\protected@file@percent }
\newlabel{biggest}{{6.4}{106}{Examples from the German idiom translation test set. \relax }{table.caption.95}{}}
\citation{sennrich-haddow-birch:2016:P16-12}
\citation{koehn-etal-2007-moses}
\BKM@entry{id=85,dest={73656374696F6E2E362E35},srcline={349}}{4964696F6D207472616E736C6174696F6E206576616C756174696F6E}
\BKM@entry{id=86,dest={73756273656374696F6E2E362E352E31},srcline={356}}{424C4555}
\citation{Papineni2001}
\BKM@entry{id=87,dest={73756273656374696F6E2E362E352E32},srcline={359}}{4D6F64696669656420756E696772616D20707265636973696F6E}
\citation{dyer-chahuneau-smith:2013:NAACL-HLT}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-12}{{107}{6.4}{section.6.4}}}
\@writefile{brf}{\backcite{koehn-etal-2007-moses}{{107}{6.4}{section.6.4}}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Idiom translation evaluation}{107}{section.6.5}\protected@file@percent }
\newlabel{idevalus}{{6.5}{107}{Idiom translation evaluation}{section.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}BLEU}{107}{subsection.6.5.1}\protected@file@percent }
\@writefile{brf}{\backcite{Papineni2001}{{107}{6.5.1}{subsection.6.5.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Modified unigram precision}{107}{subsection.6.5.2}\protected@file@percent }
\@writefile{brf}{\backcite{dyer-chahuneau-smith:2013:NAACL-HLT}{{107}{6.5.2}{subsection.6.5.2}}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Examples from the resulting test set of sentence pairs containing idiomatic expressions. NMT and PBMT translations of sentences are provided, highlighting the challenge of idiom translation. \relax }}{108}{table.caption.96}\protected@file@percent }
\newlabel{smtnmt}{{6.5}{108}{Examples from the resulting test set of sentence pairs containing idiomatic expressions. NMT and PBMT translations of sentences are provided, highlighting the challenge of idiom translation. \relax }{table.caption.96}{}}
\BKM@entry{id=88,dest={73756273656374696F6E2E362E352E33},srcline={376}}{576F72642D4C6576656C206964696F6D206163637572616379}
\BKM@entry{id=89,dest={73756273656374696F6E2E362E352E34},srcline={404}}{4576616C756174696F6E20726573756C7473}
\citation{isabelle2017challenge}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.3}Word-Level idiom accuracy}{109}{subsection.6.5.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces Translation performance on the German idiom translation test set. \textit  {Word-level Idiom Accuracy} and \textit  {Unigram Precision} are computed only on the idiom phrase and its corresponding translation in the sentence. \relax }}{109}{table.caption.97}\protected@file@percent }
\newlabel{numbers}{{6.6}{109}{Translation performance on the German idiom translation test set. \textit {Word-level Idiom Accuracy} and \textit {Unigram Precision} are computed only on the idiom phrase and its corresponding translation in the sentence. \relax }{table.caption.97}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.4}Evaluation results}{109}{subsection.6.5.4}\protected@file@percent }
\BKM@entry{id=90,dest={73656374696F6E2E362E36},srcline={437}}{436F6E636C7573696F6E}
\@writefile{brf}{\backcite{isabelle2017challenge}{{110}{6.5.4}{subsection.6.5.4}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Attention visualization of the translation of two sampled German sentences. Darker color means higher weight. The blocked area marks the idiomatic expression and its generated translation. The reference translations are: (left) \textit  {``Berlin \relax $\@@underline {\hbox {has a mind of its own}}\mathsurround \z@ $\relax  and is doing its own thing.''} and (right) \textit  {``We are therefore all \relax $\@@underline {\hbox {in the same boat}}\mathsurround \z@ $\relax , so to speak.''} \relax }}{110}{figure.caption.98}\protected@file@percent }
\newlabel{att2}{{6.2}{110}{Attention visualization of the translation of two sampled German sentences. Darker color means higher weight. The blocked area marks the idiomatic expression and its generated translation. The reference translations are: (left) \textit {``Berlin \underline {has a mind of its own} and is doing its own thing.''} and (right) \textit {``We are therefore all \underline {in the same boat}, so to speak.''} \relax }{figure.caption.98}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Conclusion}{111}{section.6.6}\protected@file@percent }
\newlabel{idconc}{{6.6}{111}{Conclusion}{section.6.6}{}}
\acronymused{rq:id1}
\acronymused{rq:id2}
\@writefile{toc}{\contentsline {paragraph}{Research Question 3:}{111}{paragraph*.99}\protected@file@percent }
\acronymused{rq:vol}
\BKM@entry{id=91,dest={636861707465722E37},srcline={2}}{566F6C6174696C6974696573206F66204E657572616C204D6F64656C73}
\BKM@entry{id=92,dest={73656374696F6E2E372E31},srcline={6}}{496E74726F64756374696F6E20616E64207265736561726368207175657374696F6E73}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Volatilities of Neural Models}{113}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:research-05}{{7}{113}{Volatilities of Neural Models}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction and research questions}{113}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Research Question 3:}{113}{paragraph*.100}\protected@file@percent }
\acronymused{rq:vol}
\acronymused{rq:vol1}
\BKM@entry{id=93,dest={73656374696F6E2E372E32},srcline={56}}{4E6F6973792074657874207472616E736C6174696F6E}
\citation{D18-1050}
\citation{li-EtAl:2019:WMT1}
\citation{DBLP:journals/corr/abs-1711-02173}
\citation{halluc}
\citation{D18-1050}
\BKM@entry{id=94,dest={73656374696F6E2E372E33},srcline={69}}{566F6C6174696C69747920696E206D616368696E65207472616E736C6174696F6E}
\acronymused{rq:vol2}
\@writefile{toc}{\contentsline {paragraph}{Organization.}{114}{paragraph*.101}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Noisy text translation}{114}{section.7.2}\protected@file@percent }
\newlabel{isitanothernoise}{{7.2}{114}{Noisy text translation}{section.7.2}{}}
\@writefile{brf}{\backcite{D18-1050}{{114}{7.2}{section.7.2}}}
\@writefile{brf}{\backcite{li-EtAl:2019:WMT1}{{114}{7.2}{section.7.2}}}
\@writefile{brf}{\backcite{DBLP:journals/corr/abs-1711-02173}{{114}{7.2}{section.7.2}}}
\@writefile{brf}{\backcite{halluc}{{114}{7.2}{section.7.2}}}
\@writefile{brf}{\backcite{D18-1050}{{114}{7.2}{section.7.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Volatility in machine translation}{114}{section.7.3}\protected@file@percent }
\newlabel{secvol}{{7.3}{114}{Volatility in machine translation}{section.7.3}{}}
\BKM@entry{id=95,dest={73656374696F6E2E372E34},srcline={115}}{566172696174696F6E2067656E65726174696F6E}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Insertion of the German word \textit  {`sehr'} (English: \textit  {`very'}) in different positions in the source sentence results in substantially different translations. Note that all source sentences are syntactically correct and semantically plausible. We use a Transformer model trained on WMT data with 6 encoder and decoder layers and 8 attention heads. $^\dagger $ indicates the original sentence from WMT 2017. \relax }}{115}{table.caption.102}\protected@file@percent }
\newlabel{first}{{7.1}{115}{Insertion of the German word \textit {`sehr'} (English: \textit {`very'}) in different positions in the source sentence results in substantially different translations. Note that all source sentences are syntactically correct and semantically plausible. We use a Transformer model trained on WMT data with 6 encoder and decoder layers and 8 attention heads. $^\dagger $ indicates the original sentence from WMT 2017. \relax }{table.caption.102}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Variation generation}{115}{section.7.4}\protected@file@percent }
\newlabel{secsentvar}{{7.4}{115}{Variation generation}{section.7.4}{}}
\citation{escude-font-costa-jussa-2019-equalizing,stanovsky-etal-2019-evaluating}
\@writefile{brf}{\backcite{escude-font-costa-jussa-2019-equalizing}{{116}{7.4}{equation.7.4.1}}}
\@writefile{brf}{\backcite{stanovsky-etal-2019-evaluating}{{116}{7.4}{equation.7.4.1}}}
\citation{bojar-EtAl:2018:WMT1}
\BKM@entry{id=96,dest={73756273656374696F6E2E372E342E31},srcline={189}}{4578706572696D656E74616C207365747570}
\citation{bojar-EtAl:2018:WMT1}
\citation{luong:2015:EMNLP}
\citation{vaswani2017attention}
\citation{sennrich-haddow-birch:2016:P16-12}
\citation{2017opennmt}
\citation{kingma2014adam}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces  Examples of different variations from WMT. [$w_i$\textbackslash $w_j$] indicates that $w_i$ in the original sentence is replaced by $w_j$. ${\phi }$ is the empty string.\relax }}{117}{table.caption.103}\protected@file@percent }
\newlabel{examplesofpert}{{7.2}{117}{Examples of different variations from WMT. [$w_i$\textbackslash $w_j$] indicates that $w_i$ in the original sentence is replaced by $w_j$. ${\phi }$ is the empty string.\relax }{table.caption.103}{}}
\@writefile{brf}{\backcite{bojar-EtAl:2018:WMT1}{{117}{7.4}{table.caption.103}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Experimental setup}{117}{subsection.7.4.1}\protected@file@percent }
\@writefile{brf}{\backcite{bojar-EtAl:2018:WMT1}{{117}{7.4.1}{subsection.7.4.1}}}
\@writefile{brf}{\backcite{luong:2015:EMNLP}{{117}{7.4.1}{subsection.7.4.1}}}
\@writefile{brf}{\backcite{vaswani2017attention}{{117}{7.4.1}{subsection.7.4.1}}}
\@writefile{brf}{\backcite{sennrich-haddow-birch:2016:P16-12}{{117}{7.4.1}{subsection.7.4.1}}}
\@writefile{lot}{\contentsline {table}{\numberline {7.3}{\ignorespaces  BLEU scores of different baseline models on the WMT news data for translation of German$\leftrightarrow $English.\relax }}{117}{table.caption.104}\protected@file@percent }
\newlabel{bleus}{{7.3}{117}{BLEU scores of different baseline models on the WMT news data for translation of German$\leftrightarrow $English.\relax }{table.caption.104}{}}
\citation{vaswani2017attention}
\citation{kingma2014adam}
\citation{2017opennmt}
\BKM@entry{id=97,dest={73656374696F6E2E372E35},srcline={217}}{556E657870656374656420616E64206572726F6E656F7573206368616E676573}
\BKM@entry{id=98,dest={73756273656374696F6E2E372E352E31},srcline={226}}{446576696174696F6E732066726F6D206F726967696E616C207472616E736C6174696F6E}
\citation{levenshtein1966binary}
\@writefile{toc}{\contentsline {paragraph}{RNN}{118}{paragraph*.105}\protected@file@percent }
\@writefile{brf}{\backcite{2017opennmt}{{118}{7.4.1}{paragraph*.105}}}
\@writefile{brf}{\backcite{kingma2014adam}{{118}{7.4.1}{paragraph*.105}}}
\@writefile{toc}{\contentsline {paragraph}{Transformer}{118}{paragraph*.106}\protected@file@percent }
\@writefile{brf}{\backcite{vaswani2017attention}{{118}{7.4.1}{paragraph*.106}}}
\@writefile{brf}{\backcite{kingma2014adam}{{118}{7.4.1}{paragraph*.106}}}
\@writefile{brf}{\backcite{2017opennmt}{{118}{7.4.1}{paragraph*.106}}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Unexpected and erroneous changes}{118}{section.7.5}\protected@file@percent }
\newlabel{secvolassess}{{7.5}{118}{Unexpected and erroneous changes}{section.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Deviations from original translation}{118}{subsection.7.5.1}\protected@file@percent }
\newlabel{ctsec}{{7.5.1}{118}{Deviations from original translation}{subsection.7.5.1}{}}
\@writefile{brf}{\backcite{levenshtein1966binary}{{118}{7.5.1}{subsection.7.5.1}}}
\BKM@entry{id=99,dest={73756273656374696F6E2E372E352E32},srcline={285}}{4F7363696C6C6174696F6E73206F6620766172696174696F6E20696E207472616E736C6174696F6E73}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces \textit  {Levenshtein distance} and \textit  {span of change} between translations of sentence variations for RNN and Transformer. The majority of sentence variations fall into the category of \textit  {minor} changes between translations (blue area). However, a surprising number of cases have significant changes (red area). RNN exhibits a slightly more unstable pattern i.e., sentence variations with large edit differences and large spans of change. \relax }}{119}{figure.caption.107}\protected@file@percent }
\newlabel{fig1}{{7.1}{119}{\textit {Levenshtein distance} and \textit {span of change} between translations of sentence variations for RNN and Transformer. The majority of sentence variations fall into the category of \textit {minor} changes between translations (blue area). However, a surprising number of cases have significant changes (red area). RNN exhibits a slightly more unstable pattern i.e., sentence variations with large edit differences and large spans of change. \relax }{figure.caption.107}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.4}{\ignorespaces An example of the generated variations of an English sentence and different sentence-level metrics for the translation of each variation. We compute the oscillation range for each sentence in the test data. \relax }}{119}{table.caption.108}\protected@file@percent }
\newlabel{oscmetexams}{{7.4}{119}{An example of the generated variations of an English sentence and different sentence-level metrics for the translation of each variation. We compute the oscillation range for each sentence in the test data. \relax }{table.caption.108}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Oscillations of variation in translations}{119}{subsection.7.5.2}\protected@file@percent }
\citation{denkowski-lavie-2011-meteor}
\citation{Snover06astudy}
\BKM@entry{id=100,dest={73756273656374696F6E2E372E352E33},srcline={329}}{54686520656666656374206F6620766F6C6174696C697479206F6E207472616E736C6174696F6E207175616C697479}
\@writefile{brf}{\backcite{denkowski-lavie-2011-meteor}{{120}{7.5.2}{subsection.7.5.2}}}
\@writefile{brf}{\backcite{Snover06astudy}{{120}{7.5.2}{subsection.7.5.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Oscillations of various sentence-level attributes for randomly sampled sentences from our test data and their \textit  {substitute number} variations. The data points are the mean values for all variations of each sentence, and the error bars indicate the range of oscillation of the metrics. The x-axis represents test sentence instances, sorted based on the corresponding metric. Ideally, each data point should have zero oscillation. \relax }}{120}{figure.caption.109}\protected@file@percent }
\newlabel{figosc}{{7.2}{120}{Oscillations of various sentence-level attributes for randomly sampled sentences from our test data and their \textit {substitute number} variations. The data points are the mean values for all variations of each sentence, and the error bars indicate the range of oscillation of the metrics. The x-axis represents test sentence instances, sorted based on the corresponding metric. Ideally, each data point should have zero oscillation. \relax }{figure.caption.109}{}}
\citation{bojar-EtAl:2016:WMT1}
\@writefile{lot}{\contentsline {table}{\numberline {7.5}{\ignorespaces  Mean oscillations for \textit  {substitute number} variations. In theory, the variations should result in zero oscillations for every metric.\relax }}{121}{table.caption.110}\protected@file@percent }
\newlabel{oscstats}{{7.5}{121}{Mean oscillations for \textit {substitute number} variations. In theory, the variations should result in zero oscillations for every metric.\relax }{table.caption.110}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}The effect of volatility on translation quality}{121}{subsection.7.5.3}\protected@file@percent }
\@writefile{brf}{\backcite{bojar-EtAl:2016:WMT1}{{121}{7.5.3}{subsection.7.5.3}}}
\@writefile{lot}{\contentsline {table}{\numberline {7.6}{\ignorespaces Definitions of different labels of changes and examples for each category. The annotators identified these differences between the translations.  \relax }}{121}{table.caption.111}\protected@file@percent }
\newlabel{guidelinestab}{{7.6}{121}{Definitions of different labels of changes and examples for each category. The annotators identified these differences between the translations.  \relax }{table.caption.111}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.7}{\ignorespaces  A random sample of sentences from the WMT test sets and our proposed variations shown with `unexpected change' annotations ($\Delta Translation$). The cases where the unexpected change leads to a change in translation quality are marked in column $\Delta Quality$. [$w_i$\textbackslash $w_j$] indicates that $w_i$ in the original sentence is replaced by $w_j$. $S$ is the original and modified source sentence, $R$ is the original and modified reference translation, $T$ is the translation of the original sentence, and $T_m$ is the translation of the modified sentence. Differences in translations related to annotations are underlined.\relax }}{122}{table.caption.112}\protected@file@percent }
\newlabel{man}{{7.7}{122}{A random sample of sentences from the WMT test sets and our proposed variations shown with `unexpected change' annotations ($\Delta Translation$). The cases where the unexpected change leads to a change in translation quality are marked in column $\Delta Quality$. [$w_i$\textbackslash $w_j$] indicates that $w_i$ in the original sentence is replaced by $w_j$. $S$ is the original and modified source sentence, $R$ is the original and modified reference translation, $T$ is the translation of the original sentence, and $T_m$ is the translation of the modified sentence. Differences in translations related to annotations are underlined.\relax }{table.caption.112}{}}
\BKM@entry{id=101,dest={73756273656374696F6E2E372E352E34},srcline={432}}{47656E6572616C697A6174696F6E20616E6420636F6D706F736974696F6E616C697479}
\citation{linzen-etal-2016-assessing,chowdhury-zamparelli-2018-rnn}
\citation{Frege1892}
\citation{montague1974d,Pelletier94theprinciple,DBLP:journals/jolli/Janssen01}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Categories of unexpected changes in the translation of sentence variations as provided by annotators. The percentage of sentence variations with \textit  {minor} and \textit  {major} edit differences, as defined in\relax \nobreakspace  {}\ref  {ctsec}, are shown separately. The hatched pattern indicates the ratio of sentence variations for which the translation quality changes.\relax }}{123}{figure.caption.113}\protected@file@percent }
\newlabel{fig2}{{7.3}{123}{Categories of unexpected changes in the translation of sentence variations as provided by annotators. The percentage of sentence variations with \textit {minor} and \textit {major} edit differences, as defined in~\ref {ctsec}, are shown separately. The hatched pattern indicates the ratio of sentence variations for which the translation quality changes.\relax }{figure.caption.113}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.4}Generalization and compositionality}{123}{subsection.7.5.4}\protected@file@percent }
\@writefile{brf}{\backcite{linzen-etal-2016-assessing}{{123}{7.5.4}{subsection.7.5.4}}}
\@writefile{brf}{\backcite{chowdhury-zamparelli-2018-rnn}{{123}{7.5.4}{subsection.7.5.4}}}
\@writefile{brf}{\backcite{Frege1892}{{123}{7.5.4}{subsection.7.5.4}}}
\@writefile{brf}{\backcite{montague1974d}{{123}{7.5.4}{subsection.7.5.4}}}
\@writefile{brf}{\backcite{Pelletier94theprinciple}{{123}{7.5.4}{subsection.7.5.4}}}
\citation{fodor2002compositionality}
\citation{DBLP:journals/corr/abs-1902-07181,babyai_iclr19}
\citation{ijcai2020-708}
\citation{886a37b5fc2f43449e4bca3b5557e3ae}
\citation{DBLP:journals/corr/abs-1904-00157}
\BKM@entry{id=102,dest={73656374696F6E2E372E36},srcline={466}}{436F6E636C7573696F6E}
\@writefile{brf}{\backcite{DBLP:journals/jolli/Janssen01}{{124}{7.5.4}{subsection.7.5.4}}}
\@writefile{brf}{\backcite{fodor2002compositionality}{{124}{7.5.4}{subsection.7.5.4}}}
\@writefile{brf}{\backcite{DBLP:journals/corr/abs-1902-07181}{{124}{7.5.4}{subsection.7.5.4}}}
\@writefile{brf}{\backcite{babyai_iclr19}{{124}{7.5.4}{subsection.7.5.4}}}
\@writefile{brf}{\backcite{ijcai2020-708}{{124}{7.5.4}{subsection.7.5.4}}}
\@writefile{brf}{\backcite{886a37b5fc2f43449e4bca3b5557e3ae}{{124}{7.5.4}{subsection.7.5.4}}}
\@writefile{brf}{\backcite{DBLP:journals/corr/abs-1904-00157}{{124}{7.5.4}{subsection.7.5.4}}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Conclusion}{124}{section.7.6}\protected@file@percent }
\newlabel{secvolconc}{{7.6}{124}{Conclusion}{section.7.6}{}}
\acronymused{rq:vol1}
\citation{886a37b5fc2f43449e4bca3b5557e3ae,cogswell2019emergence}
\acronymused{rq:vol2}
\@writefile{brf}{\backcite{886a37b5fc2f43449e4bca3b5557e3ae}{{125}{{{\textbf  {RQ3.4 }}}}{Item.50}}}
\@writefile{brf}{\backcite{cogswell2019emergence}{{125}{{{\textbf  {RQ3.4 }}}}{Item.50}}}
\@writefile{toc}{\contentsline {paragraph}{Research Question 3:}{125}{paragraph*.114}\protected@file@percent }
\acronymused{rq:vol}
\@writefile{toc}{\vspace  \bigskipamount }
\BKM@entry{id=103,dest={636861707465722E38},srcline={3}}{436F6E636C7573696F6E73}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conclusions}{127}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:conclusions}{{8}{127}{Conclusions}{chapter.8}{}}
\BKM@entry{id=104,dest={73656374696F6E2E382E31},srcline={36}}{4D61696E2066696E64696E6773}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Main findings}{128}{section.8.1}\protected@file@percent }
\acronymused{rq:topic1}
\newlabel{rq:topic1}{{{{\textbf  {RQ1.1 }}}}{128}{Main findings}{Item.51}{}}
\acronymused{rq:topic2}
\newlabel{rq:topic2}{{{{\textbf  {RQ1.2 }}}}{128}{Main findings}{Item.52}{}}
\acronymused{rq:topic3}
\newlabel{rq:topic3}{{{{\textbf  {RQ1.3 }}}}{128}{Main findings}{Item.53}{}}
\acronymused{rq:topic}
\acronymused{rq:tda1}
\newlabel{rq:tda1}{{{{\textbf  {RQ2.1 }}}}{129}{Main findings}{Item.55}{}}
\acronymused{rq:tda2}
\newlabel{rq:tda2}{{{{\textbf  {RQ2.2 }}}}{130}{Main findings}{Item.56}{}}
\acronymused{rq:bt1}
\newlabel{rq:bt1}{{{{\textbf  {RQ2.3 }}}}{130}{Main findings}{Item.57}{}}
\acronymused{rq:bt2}
\newlabel{rq:bt2}{{{{\textbf  {RQ2.4 }}}}{130}{Main findings}{Item.58}{}}
\citation{isabelle2017challenge}
\acronymused{rq:tdabt}
\newlabel{rq:tdabt}{{{RQ2}}{131}{Main findings}{Item.59}{}}
\@writefile{brf}{\backcite{isabelle2017challenge}{{131}{{RQ2}}{Item.59}}}
\acronymused{rq:id1}
\newlabel{rq:id1}{{{{\textbf  {RQ3.1 }}}}{131}{Main findings}{Item.60}{}}
\citation{shao-etal-2018-evaluating}
\acronymused{rq:id2}
\newlabel{rq:id2}{{{{\textbf  {RQ3.2 }}}}{132}{Main findings}{Item.61}{}}
\@writefile{brf}{\backcite{shao-etal-2018-evaluating}{{132}{{{\textbf  {RQ3.2 }}}}{Item.61}}}
\acronymused{rq:vol1}
\newlabel{rq:vol1}{{{{\textbf  {RQ3.3 }}}}{132}{Main findings}{Item.62}{}}
\citation{886a37b5fc2f43449e4bca3b5557e3ae}
\BKM@entry{id=105,dest={73656374696F6E2E382E32},srcline={275}}{46757475726520776F726B}
\acronymused{rq:vol2}
\newlabel{rq:vol2}{{{{\textbf  {RQ3.4 }}}}{133}{Main findings}{Item.63}{}}
\@writefile{brf}{\backcite{886a37b5fc2f43449e4bca3b5557e3ae}{{133}{{{\textbf  {RQ3.4 }}}}{Item.63}}}
\acronymused{rq:vol}
\newlabel{rq:vol}{{{RQ3}}{133}{Main findings}{Item.64}{}}
\citation{Papineni2001}
\citation{lin-2004-rouge}
\citation{banerjee-lavie-2005-meteor}
\citation{Snover06astudy}
\citation{chen-etal-2019-evaluating}
\citation{ribeiro-etal-2020-beyond}
\citation{Frege1892,fodor1992holism}
\citation{DBLP:journals/corr/abs-1711-00350,ijcai2020-708}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Future work}{134}{section.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Are evaluation metrics for generation tasks still adequate?}{134}{paragraph*.115}\protected@file@percent }
\@writefile{brf}{\backcite{Papineni2001}{{134}{8.2}{paragraph*.115}}}
\@writefile{brf}{\backcite{lin-2004-rouge}{{134}{8.2}{paragraph*.115}}}
\@writefile{brf}{\backcite{banerjee-lavie-2005-meteor}{{134}{8.2}{paragraph*.115}}}
\@writefile{brf}{\backcite{Snover06astudy}{{134}{8.2}{paragraph*.115}}}
\@writefile{brf}{\backcite{chen-etal-2019-evaluating}{{134}{8.2}{paragraph*.115}}}
\@writefile{brf}{\backcite{ribeiro-etal-2020-beyond}{{134}{8.2}{paragraph*.115}}}
\@writefile{toc}{\contentsline {paragraph}{How can we learn complex nuances and structures of language?}{134}{paragraph*.116}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How compositional are sequence-to-sequence models?}{134}{paragraph*.117}\protected@file@percent }
\@writefile{brf}{\backcite{Frege1892}{{134}{8.2}{paragraph*.117}}}
\@writefile{brf}{\backcite{fodor1992holism}{{134}{8.2}{paragraph*.117}}}
\@writefile{brf}{\backcite{DBLP:journals/corr/abs-1711-00350}{{135}{8.2}{paragraph*.117}}}
\@writefile{brf}{\backcite{ijcai2020-708}{{135}{8.2}{paragraph*.117}}}
\bibstyle{abbrvnat}
\bibdata{thesis_clean}
\BKM@entry{id=106,dest={636861707465722A2E313138},srcline={1}}{4269626C696F677261706879}
\bibcite{abdulmumin2020using}{{1}{2020}{{Abdulmumin et~al.}}{{Abdulmumin, Galadanci, and Isa}}}
\bibcite{agarwal-lavie-2008-meteor}{{2}{2008}{{Agarwal and Lavie}}{{}}}
\bibcite{agrawal-etal-2018-beating}{{3}{2018}{{Agrawal et~al.}}{{Agrawal, Chenthil~Kumar, Muralidharan, and Sharma}}}
\bibcite{aharoni-etal-2019-massively}{{4}{2019}{{Aharoni et~al.}}{{Aharoni, Johnson, and Firat}}}
\bibcite{DBLP:journals/corr/abs-1902-07181}{{5}{2019}{{Andreas}}{{}}}
\bibcite{artetxe-etal-2018-unsupervised}{{6}{2018{a}}{{Artetxe et~al.}}{{Artetxe, Labaka, and Agirre}}}
\bibcite{artetxe2018iclr}{{7}{2018{b}}{{Artetxe et~al.}}{{Artetxe, Labaka, Agirre, and Cho}}}
\bibcite{artetxe-etal-2019-effective}{{8}{2019}{{Artetxe et~al.}}{{Artetxe, Labaka, and Agirre}}}
\bibcite{D11-1033}{{9}{2011}{{Axelrod et~al.}}{{Axelrod, He, and Gao}}}
\bibcite{axelrod2015class}{{10}{2015}{{Axelrod et~al.}}{{Axelrod, Vyas, Martindale, Carpuat, and Hopkins}}}
\bibcite{Ba2016LayerN}{{11}{2016}{{Ba et~al.}}{{Ba, Kiros, and Hinton}}}
\bibcite{DBLP:journals/corr/BahdanauCB14}{{12}{2015}{{Bahdanau et~al.}}{{Bahdanau, Cho, and Bengio}}}
\bibcite{banerjee-lavie-2005-meteor}{{13}{2005}{{Banerjee and Lavie}}{{}}}
\bibcite{DBLP:journals/corr/abs-1904-00157}{{14}{2019}{{Baroni}}{{}}}
\bibcite{baroni-lenci-2010-distributional}{{15}{2010}{{Baroni and Lenci}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{137}{chapter*.118}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{baroni-etal-2014-dont}{{16}{2014}{{Baroni et~al.}}{{Baroni, Dinu, and Kruszewski}}}
\bibcite{barrault-EtAl:2019:WMT}{{17}{2019}{{Barrault et~al.}}{{Barrault, Bojar, Costa-juss{\`a}, Federmann, Fishel, Graham, Haddow, Huck, Koehn, Malmasi, Monz, M{\"u}ller, Pal, Post, and Zampieri}}}
\bibcite{DBLP:journals/corr/abs-1711-02173}{{18}{2018}{{Belinkov and Bisk}}{{}}}
\bibcite{belinkov-etal-2017-neural}{{19}{2017}{{Belinkov et~al.}}{{Belinkov, Durrani, Dalvi, Sajjad, and Glass}}}
\bibcite{Bengio:2003}{{20}{2003}{{Bengio et~al.}}{{Bengio, Ducharme, Vincent, and Janvin}}}
\bibcite{bentivogli-EtAl:2016:EMNLP2016}{{21}{2016}{{Bentivogli et~al.}}{{Bentivogli, Bisazza, Cettolo, and Federico}}}
\bibcite{blackwood-etal-2018-multilingual}{{22}{2018}{{Blackwood et~al.}}{{Blackwood, Ballesteros, and Ward}}}
\bibcite{bojar-EtAl:2016:WMT1}{{23}{2016}{{Bojar et~al.}}{{Bojar, Chatterjee, Federmann, Graham, Haddow, Huck, Jimeno~Yepes, Koehn, Logacheva, Monz, Negri, Neveol, Neves, Popel, Post, Rubino, Scarton, Specia, Turchi, Verspoor, and Zampieri}}}
\bibcite{bojar-EtAl:2017:WMT1}{{24}{2017}{{Bojar et~al.}}{{Bojar, Chatterjee, Federmann, Graham, Haddow, Huang, Huck, Koehn, Liu, Logacheva, Monz, Negri, Post, Rubino, Specia, and Turchi}}}
\bibcite{bojar-EtAl:2018:WMT1}{{25}{2018}{{Bojar et~al.}}{{Bojar, Federmann, Fishel, Graham, Haddow, Huck, Koehn, and Monz}}}
\bibcite{boyd-graber-etal-2007-topic}{{26}{2007}{{Boyd-Graber et~al.}}{{Boyd-Graber, Blei, and Zhu}}}
\bibcite{brants-etal-2007-large}{{27}{2007}{{Brants et~al.}}{{Brants, Popat, Xu, Och, and Dean}}}
\bibcite{britz-etal-2017-massive}{{28}{2017}{{Britz et~al.}}{{Britz, Goldie, Luong, and Le}}}
\bibcite{brody-lapata:2009:EACL}{{29}{2009}{{Brody and Lapata}}{{}}}
\bibcite{bruni-etal-2012-distributional}{{30}{2012}{{Bruni et~al.}}{{Bruni, Boleda, Baroni, and Tran}}}
\bibcite{burlot-yvon-2018-using}{{31}{2018}{{Burlot and Yvon}}{{}}}
\bibcite{callison-burch-etal-2006-evaluating}{{32}{2006}{{Callison-Burch et~al.}}{{Callison-Burch, Osborne, and Koehn}}}
\bibcite{callisonburch-EtAl:2007:WMT}{{33}{2007}{{Callison-Burch et~al.}}{{Callison-Burch, Fordyce, Koehn, Monz, and Schroeder}}}
\bibcite{ChaplotS18}{{34}{2018}{{Chaplot and Salakhutdinov}}{{}}}
\bibcite{DBLP:conf/bmvc/ChatfieldSVZ14}{{35}{2014}{{Chatfield et~al.}}{{Chatfield, Simonyan, Vedaldi, and Zisserman}}}
\bibcite{chen-etal-2019-evaluating}{{36}{2019}{{Chen et~al.}}{{Chen, Stanovsky, Singh, and Gardner}}}
\bibcite{chen1996empirical}{{37}{1996}{{Chen and Goodman}}{{}}}
\bibcite{cherry-etal-2018-revisiting}{{38}{2018}{{Cherry et~al.}}{{Cherry, Foster, Bapna, Firat, and Macherey}}}
\bibcite{babyai_iclr19}{{39}{2019}{{Chevalier-Boisvert et~al.}}{{Chevalier-Boisvert, Bahdanau, Lahlou, Willems, Saharia, Nguyen, and Bengio}}}
\bibcite{cho2014properties}{{40}{2014}{{Cho et~al.}}{{Cho, {van Merrienboer}, Bahdanau, and Bengio}}}
\bibcite{chowdhury-zamparelli-2018-rnn}{{41}{2018}{{Chowdhury and Zamparelli}}{{}}}
\bibcite{cogswell2019emergence}{{42}{2019}{{Cogswell et~al.}}{{Cogswell, Lu, Lee, Parikh, and Batra}}}
\bibcite{collobert2019a}{{43}{2019}{{Collobert et~al.}}{{Collobert, Hannun, and Synnaeve}}}
\bibcite{costa-jussa-fonollosa-2016-character}{{44}{2016}{{Costa-juss{\`a} and Fonollosa}}{{}}}
\bibcite{cubuk2019autoaugment}{{45}{2019}{{Cubuk et~al.}}{{Cubuk, Zoph, Mane, Vasudevan, and Le}}}
\bibcite{currey-heafield-2019-zero}{{46}{2019}{{Currey and Heafield}}{{}}}
\bibcite{currey2017copied}{{47}{2017}{{Currey et~al.}}{{Currey, Barone, and Heafield}}}
\bibcite{denkowski-lavie-2011-meteor}{{48}{2011}{{Denkowski and Lavie}}{{}}}
\bibcite{denkowski:lavie:meteor-wmt:2014}{{49}{2014}{{Denkowski and Lavie}}{{}}}
\bibcite{devlin-etal-2019-bert}{{50}{2019}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{domhan-hieber-2017-using}{{51}{2017}{{Domhan and Hieber}}{{}}}
\bibcite{duong-EtAl:2015:EMNLP}{{52}{2015}{{Duong et~al.}}{{Duong, Cohn, Bird, and Cook}}}
\bibcite{dyer-chahuneau-smith:2013:NAACL-HLT}{{53}{2013}{{Dyer et~al.}}{{Dyer, Chahuneau, and Smith}}}
\bibcite{edunov-etal-2018-understanding}{{54}{2018}{{Edunov et~al.}}{{Edunov, Ott, Auli, and Grangier}}}
\bibcite{escude-font-costa-jussa-2019-equalizing}{{55}{2019}{{Escud{\'e}~Font and Costa-juss{\`a}}}{{}}}
\bibcite{evert-kermes-2003-experiments}{{56}{2003}{{Evert and Kermes}}{{}}}
\bibcite{evert-krenn-2001-methods}{{57}{2001}{{Evert and Krenn}}{{}}}
\bibcite{fadaee-monz-2018-back}{{58}{2018}{{Fadaee and Monz}}{{}}}
\bibcite{fadaee_new}{{59}{2020}{{Fadaee and Monz}}{{}}}
\bibcite{fadaee-bisazza-monz:2017:Short2}{{60}{2017{a}}{{Fadaee et~al.}}{{Fadaee, Bisazza, and Monz}}}
\bibcite{fadaee-etal-2017-learning}{{61}{2017{b}}{{Fadaee et~al.}}{{Fadaee, Bisazza, and Monz}}}
\bibcite{L18-1148}{{62}{2018}{{Fadaee et~al.}}{{Fadaee, Bisazza, and Monz}}}
\bibcite{faruqui2016problems}{{63}{2016}{{Faruqui et~al.}}{{Faruqui, Tsvetkov, Rastogi, and Dyer}}}
\bibcite{Finkelstein:2001:PSC:371920.372094}{{64}{2001}{{Finkelstein et~al.}}{{Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, and Ruppin}}}
\bibcite{firat-etal-2016-multi}{{65}{2016}{{Firat et~al.}}{{Firat, Cho, and Bengio}}}
\bibcite{firth1957synopsis}{{66}{1957}{{Firth}}{{}}}
\bibcite{fodor1992holism}{{67}{1992}{{Fodor and Lepore}}{{}}}
\bibcite{fodor2002compositionality}{{68}{2002}{{Fodor and LePore}}{{}}}
\bibcite{Frege1892}{{69}{1892}{{Frege}}{{}}}
\bibcite{freitag-al-onaizan-2017-beam}{{70}{2017}{{Freitag and Al-Onaizan}}{{}}}
\bibcite{10.5555/177910.177914}{{71}{1994}{{Gage}}{{}}}
\bibcite{Gale1992}{{72}{1992}{{Gale et~al.}}{{Gale, Church, and Yarowsky}}}
\bibcite{ganitkevitch-etal-2013-ppdb}{{73}{2013}{{Ganitkevitch et~al.}}{{Ganitkevitch, Van~Durme, and Callison-Burch}}}
\bibcite{2017arXiv170704499G}{{74}{2017}{{Garc\'{i}a-Mart\'{i}nez et~al.}}{{Garc\'{i}a-Mart\'{i}nez, Caglayan, Aransa, Bardet, Bougares, and Barrault}}}
\bibcite{garg2019tanda}{{75}{2020}{{Garg et~al.}}{{Garg, Vu, and Moschitti}}}
\bibcite{pmlr-v70-gehring17a}{{76}{2017}{{Gehring et~al.}}{{Gehring, Auli, Grangier, Yarats, and Dauphin}}}
\bibcite{ghader-monz-2017-attention}{{77}{2017}{{Ghader and Monz}}{{}}}
\bibcite{ghader-monz-2019-intrinsic}{{78}{2019}{{Ghader and Monz}}{{}}}
\bibcite{gharbieh2016word}{{79}{2016}{{Gharbieh et~al.}}{{Gharbieh, Bhavsar, and Cook}}}
\bibcite{goodfellow6572explaining}{{80}{2015}{{Goodfellow et~al.}}{{Goodfellow, Shlens, and Szegedy}}}
\bibcite{Graves2014NeuralTM}{{81}{2014}{{Graves et~al.}}{{Graves, Wayne, and Danihelka}}}
\bibcite{gu2018universal}{{82}{2018{a}}{{Gu et~al.}}{{Gu, Hassan, Devlin, and Li}}}
\bibcite{gu-etal-2018-meta}{{83}{2018{b}}{{Gu et~al.}}{{Gu, Wang, Chen, Li, and Cho}}}
\bibcite{gulcehre2017integrating}{{84}{2017}{{Gulcehre et~al.}}{{Gulcehre, Firat, Xu, Cho, and Bengio}}}
\bibcite{2017arXiv171107893H}{{85}{2017}{{{Ha} et~al.}}{{{Ha}, {Niehues}, and {Waibel}}}}
\bibcite{Halevy:2009:UED:1525642.1525689}{{86}{2009}{{Halevy et~al.}}{{Halevy, Norvig, and Pereira}}}
\bibcite{hamp-feldweg-1997-germanet}{{87}{1997}{{Hamp and Feldweg}}{{}}}
\bibcite{hassan2011semantic}{{88}{2011}{{Hassan and Mihalcea}}{{}}}
\bibcite{NIPS2016_6469}{{89}{2016{a}}{{He et~al.}}{{He, Xia, Qin, Wang, Yu, Liu, and Ma}}}
\bibcite{He2016DeepRL}{{90}{2016{b}}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{HENRICH10.264}{{91}{2010}{{Henrich and Hinrichs}}{{}}}
\bibcite{hill-etal-2015-simlex}{{92}{2015}{{Hill et~al.}}{{Hill, Reichart, and Korhonen}}}
\bibcite{10.1142/S0218488598000094}{{93}{1998}{{Hochreiter}}{{}}}
\bibcite{hochreiter1997long}{{94}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{huang2012improving}{{95}{2012}{{Huang et~al.}}{{Huang, Socher, Manning, and Ng}}}
\bibcite{huang2018gpipe}{{96}{2019}{{Huang et~al.}}{{Huang, Cheng, Bapna, Firat, Chen, Chen, Lee, Ngiam, Le, Wu, and Chen}}}
\bibcite{ijcai2020-708}{{97}{2020}{{Hupkes et~al.}}{{Hupkes, Dankers, Mul, and Bruni}}}
\bibcite{hurtado-bodell-etal-2019-interpretable}{{98}{2019}{{Hurtado~Bodell et~al.}}{{Hurtado~Bodell, Arvidsson, and Magnusson}}}
\bibcite{ide-veronis-1998-introduction}{{99}{1998}{{Ide and V{\'e}ronis}}{{}}}
\bibcite{inoue2018data}{{100}{2018}{{Inoue}}{{}}}
\bibcite{isabelle2017challenge}{{101}{2017}{{Isabelle et~al.}}{{Isabelle, Cherry, and Foster}}}
\bibcite{Jacquemin1994ATC}{{102}{1994}{{Jacquemin}}{{}}}
\bibcite{DBLP:journals/jolli/Janssen01}{{103}{2001}{{Janssen}}{{}}}
\bibcite{jean-etal-2015-using}{{104}{2015}{{Jean et~al.}}{{Jean, Cho, Memisevic, and Bengio}}}
\bibcite{jelinek98}{{105}{1998}{{Jelinek}}{{}}}
\bibcite{johnson-etal-2017-googles}{{106}{2017}{{Johnson et~al.}}{{Johnson, Schuster, Le, Krikun, Wu, Chen, Thorat, Vi{\'e}gas, Wattenberg, Corrado, Hughes, and Dean}}}
\bibcite{Jordan2011THEEO}{{107}{2011}{{Jordan}}{{}}}
\bibcite{mandar2020}{{108}{2020}{{Joshi et~al.}}{{Joshi, Chen, Liu, Weld, Zettlemoyer, and Levy}}}
\bibcite{45801}{{109}{2017}{{Kaiser et~al.}}{{Kaiser, Nachum, Roy, and Bengio}}}
\bibcite{kajiwara-komachi-2016-building}{{110}{2016}{{Kajiwara and Komachi}}{{}}}
\bibcite{karras2019style}{{111}{2019}{{Karras et~al.}}{{Karras, Laine, and Aila}}}
\bibcite{katz-giesbrecht-2006-automatic}{{112}{2006}{{Katz and Giesbrecht}}{{}}}
\bibcite{W18-2709}{{113}{2018}{{Khayrallah and Koehn}}{{}}}
\bibcite{journals/lre/Kilgarriff97}{{114}{1997}{{Kilgarriff}}{{}}}
\bibcite{kingma2014adam}{{115}{2015}{{Kingma and Ba}}{{}}}
\bibcite{NIPS2015_5950}{{116}{2015}{{Kiros et~al.}}{{Kiros, Zhu, Salakhutdinov, Zemel, Urtasun, Torralba, and Fidler}}}
\bibcite{kishida2005property}{{117}{2005}{{Kishida}}{{}}}
\bibcite{2017opennmt}{{118}{2017}{{Klein et~al.}}{{Klein, Kim, Deng, Senellart, and Rush}}}
\bibcite{klyueva-etal-2017-neural}{{119}{2017}{{Klyueva et~al.}}{{Klyueva, Doucet, and Straka}}}
\bibcite{DBLP:journals/corr/abs-1807-07279}{{120}{2018}{{Ko{\c {c}} et~al.}}{{Ko{\c {c}}, Utlu, Senel, and {\"{O}}zaktas}}}
\bibcite{koehn2005europarl}{{121}{2005}{{Koehn}}{{}}}
\bibcite{koehn2017six}{{122}{2017}{{Koehn and Knowles}}{{}}}
\bibcite{koehn-etal-2003-statistical}{{123}{2003}{{Koehn et~al.}}{{Koehn, Och, and Marcu}}}
\bibcite{koehn-etal-2007-moses}{{124}{2007}{{Koehn et~al.}}{{Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, and Herbst}}}
\bibcite{doi:10.1093/applin/17.3.326}{{125}{1996}{{K{\"o}vecses and Szab{\'o}}}{{}}}
\bibcite{kremer2014substitutes}{{126}{2014}{{Kremer et~al.}}{{Kremer, Erk, Pad\'{o}, and Thater}}}
\bibcite{NIPS2012_4824}{{127}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{kynkaanniemi2019improved}{{128}{2019}{{Kynk\"{a}\"{a}nniemi et~al.}}{{Kynk\"{a}\"{a}nniemi, Karras, Laine, Lehtinen, and Aila}}}
\bibcite{DBLP:journals/corr/abs-1711-00350}{{129}{2017}{{Lake and Baroni}}{{}}}
\bibcite{886a37b5fc2f43449e4bca3b5557e3ae}{{130}{2018}{{Lake and Baroni}}{{}}}
\bibcite{lakew-etal-2018-comparison}{{131}{2018}{{Lakew et~al.}}{{Lakew, Cettolo, and Federico}}}
\bibcite{Lambert:2011:ITM:2132960.2132997}{{132}{2011}{{Lambert et~al.}}{{Lambert, Schwenk, Servan, and Abdul-Rauf}}}
\bibcite{lample2018unsupervised}{{133}{2018{a}}{{Lample et~al.}}{{Lample, Conneau, Denoyer, and Ranzato}}}
\bibcite{lample-etal-2018-phrase}{{134}{2018{b}}{{Lample et~al.}}{{Lample, Ott, Conneau, Denoyer, and Ranzato}}}
\bibcite{Lan2020ALBERT:}{{135}{2020}{{Lan et~al.}}{{Lan, Chen, Goodman, Gimpel, Sharma, and Soricut}}}
\bibcite{lau2014learning}{{136}{2014}{{Lau et~al.}}{{Lau, Cook, McCarthy, Gella, and Baldwin}}}
\bibcite{lee-etal-2017-fully}{{137}{2017}{{Lee et~al.}}{{Lee, Cho, and Hofmann}}}
\bibcite{halluc}{{138}{2018}{{Lee et~al.}}{{Lee, Firat, Agarwal, Fannjiang, and Sussillo}}}
\bibcite{lembersky-etal-2011-language}{{139}{2011}{{Lembersky et~al.}}{{Lembersky, Ordan, and Wintner}}}
\bibcite{levenshtein1966binary}{{140}{1966}{{Levenshtein}}{{}}}
\bibcite{levy2014dependency}{{141}{2014}{{Levy and Goldberg}}{{}}}
\bibcite{levy2015improving}{{142}{2015}{{Levy et~al.}}{{Levy, Goldberg, and Dagan}}}
\bibcite{lewis2019bart}{{143}{2020}{{Lewis et~al.}}{{Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy, Stoyanov, and Zettlemoyer}}}
\bibcite{DBLP:journals/corr/abs-2004-02181}{{144}{2020}{{Li et~al.}}{{Li, Liu, Zhu, Zhao, and Shi}}}
\bibcite{li-jurafsky:2015:EMNLP}{{145}{2015}{{Li and Jurafsky}}{{}}}
\bibcite{li-etal-2010-topic}{{146}{2010}{{Li et~al.}}{{Li, Roth, and Sporleder}}}
\bibcite{li-EtAl:2019:WMT1}{{147}{2019}{{Li et~al.}}{{Li, Michel, Anastasopoulos, Belinkov, Durrani, Firat, Koehn, Neubig, Pino, and Sajjad}}}
\bibcite{lin-2004-rouge}{{148}{2004}{{Lin}}{{}}}
\bibcite{linzen-etal-2016-assessing}{{149}{2016}{{Linzen et~al.}}{{Linzen, Dupoux, and Goldberg}}}
\bibcite{liu2016ssd}{{150}{2016}{{Liu et~al.}}{{Liu, Anguelov, Erhan, Szegedy, Reed, Fu, and Berg}}}
\bibcite{luong-etal-2013-better}{{151}{2013}{{Luong et~al.}}{{Luong, Socher, and Manning}}}
\bibcite{luong:2015:EMNLP}{{152}{2015{a}}{{Luong et~al.}}{{Luong, Pham, and Manning}}}
\bibcite{luong2014addressing}{{153}{2015{b}}{{Luong et~al.}}{{Luong, Sutskever, Le, Vinyals, and Zaremba}}}
\bibcite{markantonatou2017proceedings}{{154}{2017}{{Markantonatou et~al.}}{{Markantonatou, Ramisch, Savary, and Vincze}}}
\bibcite{Marton:2009:ISM:1699510.1699560}{{155}{2009}{{Marton et~al.}}{{Marton, Callison-Burch, and Resnik}}}
\bibcite{mccarthy2007semeval}{{156}{2007}{{McCarthy and Navigli}}{{}}}
\bibcite{Melamed98manualannotation}{{157}{1998}{{Melamed}}{{}}}
\bibcite{melamud2015simple}{{158}{2015}{{Melamud et~al.}}{{Melamud, Levy, and Dagan}}}
\bibcite{D18-1050}{{159}{2018}{{Michel and Neubig}}{{}}}
\bibcite{mikolov2013efficient}{{160}{2013{a}}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{mikolov2013distributed}{{161}{2013{b}}{{Mikolov et~al.}}{{Mikolov, Sutskever, Chen, Corrado, and Dean}}}
\bibcite{mikolov-etal-2013-linguistic}{{162}{2013{c}}{{Mikolov et~al.}}{{Mikolov, Yih, and Zweig}}}
\bibcite{miller-1985-dictionaries}{{163}{1985}{{Miller}}{{}}}
\bibcite{miller1995wordnet}{{164}{1995}{{Miller}}{{}}}
\bibcite{montague1974d}{{165}{1974}{{Montague}}{{}}}
\bibcite{moore-lewis-2010-intelligent}{{166}{2010}{{Moore and Lewis}}{{}}}
\bibcite{moussallem-etal-2018-lidioms}{{167}{2018}{{Moussallem et~al.}}{{Moussallem, Sherif, Esteves, Zampieri, and Ngonga~Ngomo}}}
\bibcite{muzny2013automatic}{{168}{2013}{{Muzny and Zettlemoyer}}{{}}}
\bibcite{Navigli2012}{{169}{2012}{{Navigli}}{{}}}
\bibcite{navigli2010babelnet}{{170}{2010}{{Navigli and Ponzetto}}{{}}}
\bibcite{neelakantan2014efficient}{{171}{2014}{{Neelakantan et~al.}}{{Neelakantan, Shankar, Passos, and McCallum}}}
\bibcite{ngo-etal-2019-overcoming}{{172}{2019}{{Ngo et~al.}}{{Ngo, Ha, Nguyen, and Nguyen}}}
\bibcite{10.2307/416483}{{173}{1994}{{Nunberg et~al.}}{{Nunberg, Sag, and Wasow}}}
\bibcite{ostling2017neural}{{174}{2017}{{{\"O}stling and Tiedemann}}{{}}}
\bibcite{6802355}{{175}{2015}{{{Paisley} et~al.}}{{{Paisley}, {Wang}, {Blei}, and {Jordan}}}}
\bibcite{Papineni2001}{{176}{2002}{{Papineni et~al.}}{{Papineni, Roukos, Ward, and Zhu}}}
\bibcite{Pelletier94theprinciple}{{177}{1994}{{Pelletier}}{{}}}
\bibcite{pennington2014glove}{{178}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{peters-etal-2018-deep}{{179}{2018}{{Peters et~al.}}{{Peters, Neumann, Iyyer, Gardner, Clark, Lee, and Zettlemoyer}}}
\bibcite{pham2017karlsruhe}{{180}{2017}{{Pham et~al.}}{{Pham, Niehues, Ha, Cho, Sperber, and Waibel}}}
\bibcite{2018arXiv180406189P}{{181}{2018}{{{Poncelas} et~al.}}{{{Poncelas}, {Shterionov}, {Way}, {Maillette de Buy Wenniger}, and {Passban}}}}
\bibcite{2019arXiv190607808P}{{182}{2019{a}}{{{Poncelas} et~al.}}{{{Poncelas}, {Maillette de Buy Wenniger}, and {Way}}}}
\bibcite{DBLP:journals/corr/abs-1909-03750}{{183}{2019{b}}{{{Poncelas} et~al.}}{{{Poncelas}, Popovic, Shterionov, de~Buy~Wenniger, and Way}}}
\bibcite{qi-etal-2018-pre}{{184}{2018}{{Qi et~al.}}{{Qi, Sachan, Felix, Padmanabhan, and Neubig}}}
\bibcite{qiu-tu-yu:2016:EMNLP2016}{{185}{2016}{{Qiu et~al.}}{{Qiu, Tu, and Yu}}}
\bibcite{radford2019language}{{186}{2019}{{Radford et~al.}}{{Radford, Wu, Child, Luan, Amodei, and Sutskever}}}
\bibcite{Radinsky:2011:WTC:1963405.1963455}{{187}{2011}{{Radinsky et~al.}}{{Radinsky, Agichtein, Gabrilovich, and Markovitch}}}
\bibcite{DBLP:journals/corr/RanzatoCAZ15}{{188}{2016}{{Ranzato et~al.}}{{Ranzato, Chopra, Auli, and Zaremba}}}
\bibcite{Rapp:2009:BSA:1667583.1667625}{{189}{2009}{{Rapp}}{{}}}
\bibcite{reisinger-mooney:2010:NAACLHLT}{{190}{2010}{{Reisinger and Mooney}}{{}}}
\bibcite{ribeiro-etal-2020-beyond}{{191}{2020}{{Ribeiro et~al.}}{{Ribeiro, Wu, Guestrin, and Singh}}}
\bibcite{rios-etal-2018-word}{{192}{2018}{{Rios et~al.}}{{Rios, M{\"u}ller, and Sennrich}}}
\bibcite{Rubenstein:1965:CCS:365628.365657}{{193}{1965}{{Rubenstein and Goodenough}}{{}}}
\bibcite{10.5555/65669.104451}{{194}{1988}{{Rumelhart et~al.}}{{Rumelhart, Hinton, and Williams}}}
\bibcite{salehi-cook-2013-predicting}{{195}{2013}{{Salehi and Cook}}{{}}}
\bibcite{salehi-etal-2015-word}{{196}{2015}{{Salehi et~al.}}{{Salehi, Cook, and Baldwin}}}
\bibcite{salton-etal-2014-evaluation}{{197}{2014{a}}{{Salton et~al.}}{{Salton, Ross, and Kelleher}}}
\bibcite{salton-ross-kelleher:2014:HyTra}{{198}{2014{b}}{{Salton et~al.}}{{Salton, Ross, and Kelleher}}}
\bibcite{salton-etal-2016-idiom}{{199}{2016}{{Salton et~al.}}{{Salton, Ross, and Kelleher}}}
\bibcite{Schenk:1986:IRM:991365.991458}{{200}{1986}{{Schenk}}{{}}}
\bibcite{Schmidhuber:HabilitationThesis}{{201}{1993}{{Schmidhuber}}{{}}}
\bibcite{Schwenk2008InvestigationsOL}{{202}{2008}{{Schwenk}}{{}}}
\bibcite{sennrich-zhang-2019-revisiting}{{203}{2019}{{Sennrich and Zhang}}{{}}}
\bibcite{sennrich-etal-2016-controlling}{{204}{2016{a}}{{Sennrich et~al.}}{{Sennrich, Haddow, and Birch}}}
\bibcite{sennrich-haddow-birch:2016:P16-11}{{205}{2016{b}}{{Sennrich et~al.}}{{Sennrich, Haddow, and Birch}}}
\bibcite{sennrich-haddow-birch:2016:P16-12}{{206}{2016{c}}{{Sennrich et~al.}}{{Sennrich, Haddow, and Birch}}}
\bibcite{2017arXiv170800726S}{{207}{2017}{{Sennrich et~al.}}{{Sennrich, Birch, Currey, Germann, Haddow, Heafield, Miceli~Barone, and Williams}}}
\bibcite{shao-etal-2018-evaluating}{{208}{2018}{{Shao et~al.}}{{Shao, Sennrich, Webber, and Fancellu}}}
\bibcite{silva-etal-2018-extracting}{{209}{2018}{{Silva et~al.}}{{Silva, Liu, Poncelas, and Way}}}
\bibcite{singh2018sniper}{{210}{2018}{{Singh et~al.}}{{Singh, Najibi, and Davis}}}
\bibcite{small2013lexical}{{211}{2013}{{Small et~al.}}{{Small, Cottrell, and Tanenhaus}}}
\bibcite{Snover06astudy}{{212}{2006}{{Snover et~al.}}{{Snover, Dorr, Schwartz, Micciulla, and Makhoul}}}
\bibcite{sprent2016applied}{{213}{2016}{{Sprent and Smeeton}}{{}}}
\bibcite{stanovsky-etal-2019-evaluating}{{214}{2019}{{Stanovsky et~al.}}{{Stanovsky, Smith, and Zettlemoyer}}}
\bibcite{steinberger2006jrc}{{215}{2006}{{Steinberger et~al.}}{{Steinberger, Pouliquen, Widiger, Ignat, Erjavec, Tufis, and Varga}}}
\bibcite{sun-etal-2020-knowledge}{{216}{2020}{{Sun et~al.}}{{Sun, Wang, Chen, Utiyama, Sumita, and Zhao}}}
\bibcite{sutskever2014sequence}{{217}{2014}{{Sutskever et~al.}}{{Sutskever, Vinyals, and Le}}}
\bibcite{DBLP:conf/emnlp/SzarvasBH13}{{218}{2013}{{Szarvas et~al.}}{{Szarvas, Busa-Fekete, and H\"{u}llermeier}}}
\bibcite{tang2014learning}{{219}{2014}{{Tang et~al.}}{{Tang, Wei, Yang, Zhou, Liu, and Qin}}}
\bibcite{citeulike:635668}{{220}{2005}{{Teh et~al.}}{{Teh, Jordan, Beal, and Blei}}}
\bibcite{teh2006hierarchical}{{221}{2006}{{Teh et~al.}}{{Teh, Jordan, Beal, and Blei}}}
\bibcite{tiedemann-etal-2016-phrase}{{222}{2016}{{Tiedemann et~al.}}{{Tiedemann, Cap, Kanerva, Ginter, Stymne, {\"O}stling, and Weller-Di~Marco}}}
\bibcite{torabi-asr-etal-2018-querying}{{223}{2018}{{Torabi~Asr et~al.}}{{Torabi~Asr, Zinkov, and Jones}}}
\bibcite{tran-etal-2018-importance}{{224}{2018}{{Tran et~al.}}{{Tran, Bisazza, and Monz}}}
\bibcite{Ueffing2007}{{225}{2007}{{Ueffing et~al.}}{{Ueffing, Haffari, and Sarkar}}}
\bibcite{7b54165e73a3424b8820136bcf61ca89}{{226}{2008}{{{van der Maaten} and Hinton}}{{}}}
\bibcite{vanderwees-bisazza-monz:2017:EMNLP2017}{{227}{2017}{{van~der Wees et~al.}}{{van~der Wees, Bisazza, and Monz}}}
\bibcite{vaswani2017attention}{{228}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{wang-etal-2013-edit}{{229}{2013}{{Wang et~al.}}{{Wang, Wong, Chao, Xing, Lu, and Trancoso}}}
\bibcite{wang-etal-2019-dynamically}{{230}{2019{a}}{{Wang et~al.}}{{Wang, Caswell, and Chelba}}}
\bibcite{wang-etal-2018-switchout}{{231}{2018}{{Wang et~al.}}{{Wang, Pham, Dai, and Neubig}}}
\bibcite{Wang2019UsingDE}{{232}{2019{b}}{{Wang et~al.}}{{Wang, Cui, and Zhang}}}
\bibcite{NIPS2017_7278}{{233}{2017}{{Wang et~al.}}{{Wang, Ramanan, and Hebert}}}
\bibcite{wiseman-rush-2016-sequence}{{234}{2016}{{Wiseman and Rush}}{{}}}
\bibcite{wu2016google}{{235}{2016}{{Wu et~al.}}{{Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun, Cao, Gao, Macherey, Klingner, Shah, Johnson, Liu, Kaiser, Gouws, Kato, Kudo, Kazawa, Stevens, Kurian, Patil, Wang, Young, Smith, Riesa, Rudnick, Vinyals, Corrado, Hughes, and Dean}}}
\bibcite{wubben-etal-2012-sentence}{{236}{2012}{{Wubben et~al.}}{{Wubben, van~den Bosch, and Krahmer}}}
\bibcite{8049322}{{237}{2017}{{{Xiong} et~al.}}{{{Xiong}, {Droppo}, {Huang}, {Seide}, {Seltzer}, {Stolcke}, {Yu}, and {Zweig}}}}
\bibcite{yang-etal-2018-unsupervised}{{238}{2018}{{Yang et~al.}}{{Yang, Chen, Wang, and Xu}}}
\bibcite{yao2011nonparametric}{{239}{2011}{{Yao and Van~Durme}}{{}}}
\bibcite{yih-qazvinian-2012-measuring}{{240}{2012}{{Yih and Qazvinian}}{{}}}
\bibcite{yu-etal-2017-refining}{{241}{2017}{{Yu et~al.}}{{Yu, Wang, Lai, and Zhang}}}
\bibcite{yuan2019objectcontextual}{{242}{2019}{{Yuan et~al.}}{{Yuan, Chen, and Wang}}}
\bibcite{zhang-zong-2016-exploiting}{{243}{2016}{{Zhang and Zong}}{{}}}
\bibcite{zhang-lapata-2017-sentence}{{244}{2017}{{Zhang and Lapata}}{{}}}
\bibcite{zhang2019semanticsaware}{{245}{2019}{{Zhang et~al.}}{{Zhang, Wu, Zhao, Li, Zhang, Zhou, and Zhou}}}
\bibcite{Zhang2020RetrospectiveRF}{{246}{2020}{{Zhang et~al.}}{{Zhang, jie Yang, and Zhao}}}
\bibcite{zoph-EtAl:2016:EMNLP2016}{{247}{2016}{{Zoph et~al.}}{{Zoph, Yuret, May, and Knight}}}
\bibcite{zou2013bilingual}{{248}{2013}{{Zou et~al.}}{{Zou, Socher, Cer, and Manning}}}
\BKM@entry{id=107,dest={636861707465722A2E313139},srcline={28}}{53756D6D617279}
\@writefile{toc}{\contentsline {chapter}{Summary}{157}{chapter*.119}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\BKM@entry{id=108,dest={636861707465722A2E313230},srcline={57}}{53616D656E76617474696E67}
\@writefile{toc}{\contentsline {chapter}{Samenvatting}{159}{chapter*.120}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
